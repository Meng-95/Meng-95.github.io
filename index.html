<!DOCTYPE html>
<html lang="en">

<head>
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=UA-111713571-1"></script>
	<script>
  		window.dataLayer = window.dataLayer || [];
  		function gtag(){dataLayer.push(arguments);}
  		gtag('js', new Date());
  		gtag('config', 'UA-111713571-1');
	</script>

	<!-- Show more content -->
	<script type="text/javascript">
		function toggle_vis(id) {
	    // var e = document.getElementById(id);
	    var e = document.getElementsByClassName(id);
			var showText = document.getElementById("showText");
			for (var i = 0; i < e.length; i++) {
		    	if (e[i].style.display == "none") {
		        	e[i].style.display = "inline";
	    			showText.innerHTML = "[Show less]";
		    	} else {
		    		e[i].style.display = "none";
		    		showText.innerHTML = "[Show more]";
		    	}
		    }
	    }
	</script>

	<!-- Required meta tags -->
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

	<!-- Bootstrap core CSS -->
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/css/bootstrap.min.css" integrity="sha384-/Y6pD6FV/Vv2HJnA6t+vslU6fwYXjCFtcEpHbNJ0lyAFsXTsjBbfaDjzALeQsN6M" crossorigin="anonymous">
	<!-- https://fontawesome.com/cheatsheet -->
	<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons/css/academicons.min.css">
  <link rel="stylesheet" href="static/styles.css">
	<!-- Custom styles for this template -->
	<link href="files/jumbotron.css" rel="stylesheet">
	<script src="js/main.js"></script>
  <script src="js/scroll.js"></script>
</head>

<title>Yunzhu Li</title>

<body>
	<nav class="navbar navbar-expand-md navbar-dark fixed-top bg-dark" id="Home">
	    <div class="container">
		<a class="navbar-brand" href="#Home">Yunzhu Li</a>

		<div class="collapse navbar-collapse" id="navbarToggle">
			<ul class="navbar-nav ml-auto">
				<li class="nav-item">
					<a class="nav-link" href="#Home">Home</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="#Publications">Publications</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="#Honors">Honors</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="http://accessibility.mit.edu" target="_blank">Accessibility</a>
				</li>
			</ul>
		</div>
	    </div>
	</nav>

	<div class="container" style="padding-top: 20px; font-size: 17px">
		<div class="row">
			<div class="col-md-3", style="padding-right: 40px">
				<br>
				<img class="img-responsive img-rounded" src="files/profile.jpeg" alt="Photo" style="max-width: 100%; border:1px solid black"><br>
			</div>

			<div class="col-md-9">
			<br>
			<p>I am a Postdoctoral Scholar at <a href="https://svl.stanford.edu/" target="_blank">Stanford Vision and Learning Lab (SVL)</a>, working with <a href="https://profiles.stanford.edu/fei-fei-li" target='_blank'>Fei-Fei Li</a> and <a href="https://jiajunwu.com/" target="_blank">Jiajun Wu</a>. I received my PhD from <a href="http://www.csail.mit.edu/" target="_blank">Computer Science and Artificial Intelligence Laboratory (CSAIL)</a> at  <a href="http://www.mit.edu/" target="_blank">MIT</a>, advised by <a href="http://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a> and <a href="https://groups.csail.mit.edu/locomotion/russt.html" target="_blank">Russ Tedrake</a>, and my bachelor's degree from <a href="http://english.pku.edu.cn/" target="_blank">Peking University</a>.
			</p>

			<p>My research interests lie in robotics, computer vision, and machine learning. In particular, I am interested in enabling robots and embodied agents to better perceive and interact with the physical world.
			</p>
			
			<p>
			<a target="_blank" href="https://scholar.google.com/citations?user=WlA92lcAAAAJ&hl=en"><font color="black"><i class="ai ai-google-scholar ai-lg"></i></font></a>&nbsp;&nbsp;&nbsp;/&nbsp;&nbsp;
			<a target="_blank" href="https://github.com/yunzhuli"><font color="black"><i class="fab fa-github fa-lg"></i></font></a>&nbsp;&nbsp;&nbsp;/&nbsp;&nbsp;
			<a target="_blank" href="https://twitter.com/YunzhuLiYZ"><font color="black"><i class="fab fa-twitter fa-lg"></i></font></a>&nbsp;&nbsp;&nbsp;/&nbsp;&nbsp;
			liyunzhu [at] stanford (dot) edu
			</p>

			</div>
		</div>
	</div><br>


  <!-- News -->
	<div class="container">
		<h3 id="News" style="padding-top: 80px; margin-top: -80px;">News</h3>
		<ul>
			<!--
			<li>
				<font color="firebrick">[Upcoming]</font> Invited Talk at <b>Carnegie Mellon University</b>.
			</li>
			-->
			<li>
				I will join the <a href="https://cs.illinois.edu/" target="_blank">Department of Computer Science</a> and be affiliated with the <a href="https://ece.illinois.edu/" target="_blank">Department of Electrical & Computer Engineering</a> at the <a href="https://illinois.edu/" target="_blank">University of Illinois at Urbana-Champaign (UIUC)</a> as an <font color="firebrick">Assistant Professor</font> in Fall 2023. My group focuses on <font color="firebrick">Robot Learning</font> and aims at expanding robots' physical interaction capabilities through the following three lenses.
				<ul>
					<li><font color="firebrick">Intuitive Physics</font> (i.e., learning structured world models for robotic manipulation of objects with complicated properties).</li>
					<li><font color="firebrick">Embodied Intelligence</font> with focuses on long-horizon planning, generalization to diverse environments, and sim-to-real transfer.</li>
					<li><font color="firebrick">Multi-Modal Perception</font> for fine-grained and effective manipulation using vision, touch, audio, and language.
				</ul>
				<br>
				<b>Prospective Students:</b> I have PhD and Postdoc openings in my group at UIUC starting Fall 2023. I'm looking for technically strong and self-motivated students interested in advancing the frontiers of robot learning. Check out the following materials for an overview of my research.<br>
				<ul>
					<li><a href="https://youtu.be/8eof6F4-r0k" target="_blank">PhD Thesis Defense</a> / <a href="research_statement_UIUC_CS.pdf" target="_blank">Research Statement</a>
				</ul>
				For PhD applicants, please submit your <a href="https://grad.illinois.edu/admissions/apply" target="_blank">application</a>, select the Computer Science PhD program, and mention me as one of your Faculty of Interest.
			</li><br>

			<li>
				I will serve as an Area Chair of <a href="https://www.corl2023.org/" target="_blank">CoRL 2023</a>.
			</li>
			<li>
				I am co-instructing <a href="http://cs231n.stanford.edu/" target="_blank">CS231n: Deep Learning for Computer Vision</a> with <a href="https://profiles.stanford.edu/fei-fei-li" target='_blank'>Fei-Fei Li</a> and <a href="https://ai.stanford.edu/~rhgao/" target="_blank">Ruohan Gao</a> at Stanford.
			</li>
			<li>
				Our <a href="http://stag.csail.mit.edu/" target="_blank">scalable tactile glove</a> introduced in a <font color="firebrick">Nature</font> 2019 paper is collected by the <a href="https://mitmuseum.mit.edu/" target="_blank">MIT Museum</a>. <a href="projects/stag/stag_mit_museum.jpg" target="_blank">[Photo]</a>
			</li>
			<li>
				I have been chosen as the <font color="firebrick">First Place Recipient</font> of the Ernst A. Guillemin Master's Thesis Award in Artificial Intelligence and Decision Making at MIT.
			</li>
			<li>
				Our paper on learning 3D representations using NeRF for visuomotor control is accepted to CoRL as <font color="firebrick">Oral (Top 6.5%)</font>. <a href="https://3d-representation-learning.github.io/nerf-dy/" target="_blank">[Project]</a> <a href="https://openreview.net/forum?id=zv3NYgRZ7Qo" target="_blank">[OpenReview]</a>
			</li>
			<li>
				Our <font color="firebrick">Nature Electronics</font> paper on <a href="http://senstextile.csail.mit.edu/" target="_blank">learning human-environment interactions using conformal tactile textiles</a> is featured on the <a href="https://www.nature.com/natelectron/volumes/4/issues/3" target='_blank'>cover</a> of the issue!
			</li>
		</ul>
	</div><br><br>


	<!-- Talks -->
	<div class="container">
		<h3 id="Talks" stype="padding-top: 80px; margin-top: -80px;">
			Recent Talks
		</h3>
		<hr>

		<div class="row">
			<div class="col-md-2">
				<a href="https://www.youtube.com/watch?v=ost49rbnI_8&t=7207s" target="_blank"><img class="img-fluid img-rounded" src="talks/20220617_ThesisDefense/logo_2.jpg" style="border:1px solid black" alt=""></a>
			</div>

			<div class="col-md-10">
				Learning Structured World Models From and For Physical Interactions	
				<br>
				<b><font color="black">PhD Thesis Defense at MIT CSAIL</font></b>
				<br>
				[2022/06] <a href="https://youtu.be/8eof6F4-r0k" target="_blank">[Recording]</a>
			</div>
		</div><hr>

		<ul>
			<li>[2023/06] Invited Talk at <b>CVPR 2023 Workshop on <a href="https://sites.google.com/view/ieeecvf-cvpr2023-precognition" target="_blank">Precognition: Seeing through the Future</a></b></li>
			<li>[2023/05] Invited Talk at <b>ICRA 2023 Workshop on <a href="https://deformable-workshop.github.io/icra2023/" target="_blank">Representing and Manipulating Deformable Objects</a></b></li>
			<li>[2022/12] Guest Lecture at <b>NYU</b>, Invited Talk at <b>Shanghai AI Laboratory</b></li>
			<li>[2022/10] Invited Talk at <b>Stanford</b> and <b>UC Berkeley</b></li>
			<li>[2022/07] Invited Talk at <b>RSS 2022 Workshop on <a href="https://imrss2022.github.io/" target="_blank">Implicit Representations for Robotic Manipulation</a></b> <a href="https://www.youtube.com/watch?v=gawSKjA9skU" target="_blank">[Recording]</a> and <b>ISEE AI</b></li>
			<li>[2022/04] Invited Talk at <b>Google Research</b></li>
			<li>[2022/01] Invited Talk at <b>TU Berlin</b></li>
			<div class="talks" style="display:none">
				<li>[2021/12] Invited Talk at <b>Meta AI Research</b></li>
				<li>[2021/11] Invited Talk at <b>Toyota Research Institute</b></li>
				<li>[2021/08] Invited Talk at <b>Wayve.ai</b> and <b>Tsinghua University</b></li>
				<li>[2021/06] Invited Talk at <b>CVPR 2021 Tutorial on <a href="https://xiaolonw.github.io/graphnnv3/" target="_blank">Learning Representations via Graph-structured Networks</a></b> <a href="https://www.youtube.com/watch?v=ost49rbnI_8&t=7207s" target="_blank">[Recording]</a></li>
				<li>[2021/05] Invited Talk at <b>ICLR 2021 Workshop on <a href="https://simdl.github.io/overview/" target="_blank">Deep Learning for Simulation</a></b> <a href="https://youtu.be/sfblDZeyjk8" target="_blank">[Recording]</a></li>
				<li>[2021/04] Invited Talk at <b>Brown University</b></li>
				<li>[2021/03] Invited Talk at <b>Extrality.ai</b></li>
				<li>[2020/10] Invited Talk at <b>Stanford</b> and <b>UMass Amherst</b></li>
				<li>[2020/09] Invited Talk at the <b>University of Toronto</b> <a href="https://www.youtube.com/watch?v=y_j53bkKzq8" target="_blank">[Recording]</a></li>
			</div>
			<li style="font-size: 16px">
				<a href="javascript:toggle_vis('talks')" id="showText">[Show more]</a>
			</li>
		</ul>

		<!--
		<div class="row">
			<div class="col-md-2">
				<a href="https://www.youtube.com/watch?v=ost49rbnI_8&t=7207s" target="_blank"><img class="img-fluid img-rounded" src="talks/20210620_CVPR21_GraphTutorial/logo.jpg" style="border:1px solid black" alt=""></a>
			</div>

			<div class="col-md-10">
				Graph-Structured Networks for Physical Inference and Model-Based Control
				<br>
				Invited talk at <b><font color="black">CVPR 2021 Tutorial <a href="https://xiaolonw.github.io/graphnnv3/" target="_blank">Learning Representations via Graph-structured Networks</a></font></b>
				<br>
				<a href="https://www.youtube.com/watch?v=ost49rbnI_8&t=7207s" target="_blank">[YouTube]</a> (June 2021)
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-2">
				<a href="https://youtu.be/sfblDZeyjk8" target="_blank"><img class="img-fluid img-rounded" src="talks/20210507_ICLR21_simDL/logo.jpg" style="border:1px solid black" alt=""></a>
			</div>

			<div class="col-md-10">
				Learning Compositional Dynamics Models for Physical Inference and Model-Based Control
				<br>
				Invited talk at <b><font color="black">ICLR 2021 Workshop <a href="https://simdl.github.io/overview/" target="_blank">Deep Learning for Simulation</a></font></b>
				<br>
				<a href="https://youtu.be/sfblDZeyjk8" target="_blank">[YouTube]</a> (May 2021)
			</div>
		</div><hr>
		--!>
	</div><br><br>



	<!-- Publications -->
	<div class="container">
		<h3 id="Publications" style="padding-top: 80px; margin-top: -80px;">
			Publications
			<small><small>
			(<a href="" id="select0" onclick="showPubs(0); return false;">show selected</a> /
			 <a href="" id="select1" onclick="showPubs(1); return false;">show by date</a> /
       			 <a href="" id="select2" onclick="showPubs(2); return false;">show by topic</a>)
			</small></small><br>

			<small><small>
		  	<font color="black">Research Topics:
				<a href="#dynam" onclick="showPubs(2)">Robotic Manipulation</a> /
				<a href="#phys" onclick="showPubs(2)">Physical Scene Understanding</a> /
				<a href="#multi" onclick="showPubs(2)">Multi-Modal Perception</a> /
				<a href="#imi" onclick="showPubs(2)">Imitation Learning</a>
			</font><br>
			</small></small>
		</h3>

		<div id="pubs"></div>

		<script id="pubs_selected" language="text">
			<font color="black">(* indicates equal contribution)</font><br><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/dyn-res/dyn-res.gif" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://wangyixuan12.github.io/" target="_blank">Yixuan Wang</a>*,
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>*,
				<a href="https://krdc.web.illinois.edu/" target="_blank">Katherine Driggs-Campbell</a>,
				<a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>, and
				<a href="http://jiajunwu.com" target="_blank">Jiajun Wu</a>
				<br>
				<b><font color="black">Dynamic-Resolution Model Learning for Object Pile Manipulation</font></b><br>
				<b><a href="https://roboticsconference.org/" target="_blank">RSS 2023</a></b>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/comp_nerf_dy/combined.gif" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://dannydriess.github.io/" target="_blank">Danny Driess</a>,
				<a href="https://sites.google.com/view/zhiao-huang" target="_blank">Zhiao Huang</a>,
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>,
				<a href="https://groups.csail.mit.edu/locomotion/russt.html" target="_blank">Russ Tedrake</a>, and
				<a href="https://www.user.tu-berlin.de/mtoussai/" target="_blank">Marc Toussaint</a>
				<br>
				<b><font color="black">Learning Multi-Object Dynamics with Compositional Neural Radiance Fields</font></b><br>
				<b><a href="https://corl2022.org/" target="_blank">CoRL 2022</a></b>,
				<a href="https://dannydriess.github.io/compnerfdyn/index.html" target="_blank"> <small>[Project & Video]</small></a>
				<a href="https://openreview.net/forum?id=qUvTmyGpnm7" target="_blank"> <small>[Paper]</small></a>
				<a href="projects/comp_nerf_dy/comp_nerf_dy.bib" target="_blank"> <small>[BibTex]</small></a><br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/robocraft/robocraft.gif" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://hshi74.github.io/" target="_blank">Haochen Shi</a>*,
				<a href="http://hxu.rocks/" target="_blank">Huazhe Xu</a>*,
				<a href="https://sites.google.com/view/zhiao-huang" target="_blank">Zhiao Huang</a>,
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>, and
				<a href="http://jiajunwu.com" target="_blank">Jiajun Wu</a>
				<br>
				<b><font color="black">RoboCraft: Learning to See, Simulate, and Shape Elasto-Plastic Objects with Graph Networks</font></b><br>
				<b><a href="https://roboticsconference.org/" target="_blank">RSS 2022</a></b>,
				<a href="http://hxu.rocks/robocraft/" target="_blank"> <small>[Project & Video]</small></a>
				<a href="https://arxiv.org/abs/2205.02909" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/hshi74/RoboCraft" target="_blank"> <small>[Code]</small></a>
				<a href="projects/robocraft/robocraft.bib" target="_blank"> <small>[BibTex]</small></a><br>
				Abridged in <b>ICRA 2022</b> workshop on Representing and Manipulating Deformable Objects <a href="https://deformable-workshop.github.io/icra2022/" target="_blank"><small>[Link]</small></a><br>
				<small>Covered by</small>
				<a href="https://news.mit.edu/2022/robots-play-play-dough-0623" target="_blank"> <small>[MIT News]</small></a>
				<a href="https://www.newscientist.com/article/2325970-ai-powered-robot-learned-to-make-letters-out-of-play-doh-on-its-own/" target="_blank"> <small>[NewScientist]</small></a>
				<a href="https://techcrunch.com/2022/06/23/a-quick-trip-to-mars/" target="_blank"> <small>[TechCrunch]</small></a>
				<a href="https://hai.stanford.edu/news/training-robot-shape-letters-play-doh" target="_blank"> <small>[Stanford HAI]</small></a>

				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/nerf-dy/nerf-dy-multiview.gif" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>*,
				<a href="https://people.csail.mit.edu/lishuang/" target="_blank">Shuang Li</a>*,
				<a href="https://vsitzmann.github.io/" target="_blank">Vincent Sitzmann</a>,
				<a href="https://people.csail.mit.edu/pulkitag/" target="_blank">Pulkit Agrawal</a>, and
				<a href="http://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>
				<br>
				<b><font color="black">3D Neural Scene Representations for Visuomotor Control</font></b><br>
				<b><a href="https://www.robot-learning.org/" target="_blank">CoRL 2021</a></b>,
				<a href="https://3d-representation-learning.github.io/nerf-dy/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/abs/2107.04004" target="_blank"> <small>[Paper]</small></a>
				<a href="https://youtu.be/ELPMiifELGc" target="_blank"> <small>[Video]</small></a>
				<a href="https://openreview.net/forum?id=zv3NYgRZ7Qo" target="_blank"> <small>[OpenReview]</small></a>
				<a href="https://3d-representation-learning.github.io/nerf-dy/nerf-dy.bib" target="_blank"> <small>[BibTex]</small></a><br>
				<font color="firebrick"><b>Oral Presentation (Top 6.5%)</b></font><br>
				Abridged in <b>RSS 2021</b> workshop on Visual Learning and Reasoning for Robotics <a href="https://rssvlrr.github.io/" target="_blank"><small>[Link]</small></a><br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/senstextile/senstextile.jpg" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
        			<a href="https://yyueluo.com/" target="_blank">Yiyue Luo</a>,
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>,
        			<a href="https://pratyushasharma.github.io/" target="_blank">Pratyusha Sharma</a>,
        			<a href="https://www.csail.mit.edu/person/wan-shou" target="_blank">Wan Shou</a>,
        			<a href="http://people.csail.mit.edu/kuiwu" target="_blank">Kui Wu</a>,
        			<a href="https://www.csail.mit.edu/person/michael-foshey" target="_blank">Michael Foshey</a>,
        			<a href="https://www.csail.mit.edu/person/beichen-li" target="_blank">Beichen Li</a>,
        			<a href="http://www-mtl.mit.edu/wpmu/tpalacios/" target="_blank">Tomas Palacios</a>,
        			<a href="https://groups.csail.mit.edu/vision/torralbalab/" target="_blank">Antonio Torralba</a>, and
        			<a href="https://cdfg.mit.edu/wojciech" target="_blank">Wojciech Matusik</a>
				<br>
				<b><font color="black">Learning Human-environment Interactions using Conformal Tactile Textiles</font></b><br>
				<b><a href="https://www.nature.com/natelectron/" target="_blank">Nature Electronics</a></b> 4, 193–201 (2021),
				<font color="firebrick"><b>5-year Impact Factor: 33.695</b></font>
				<br>
				<a href="http://senstextile.csail.mit.edu/" target="_blank"> <small>[Project]</small></a>
				<a href="https://www.nature.com/articles/s41928-021-00558-0" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/YunzhuLi/senstextile" target="_blank"> <small>[Code]</small></a>
				<a href="projects/senstextile/senstextile.bib" target="_blank"> <small>[BibTex]</small></a>
				<br>
				<small>Featured on the</small>
				<a href="https://www.nature.com/natelectron/volumes/4/issues/3" target="_blank"> <small>cover</small></a>
				<small>of the issue.</small>
				<small>Editorial comments</small>
				<a href="https://www.nature.com/articles/s41928-021-00567-z" target="_blank"> <small>[Link]</small></a>
				<br>
				<small>Covered by</small>
				<a href="https://www.nature.com/articles/s41928-021-00560-6" target="_blank"> <small>[Nature Electronics News & Views]</small></a>
				<a href="https://www.csail.mit.edu/news/smart-clothes-can-measure-your-movements" target="_blank"> <small>[MIT CSAIL News]</small></a>
				<a href="https://gizmodo.com/researchers-might-have-finally-cracked-smart-clothing-1846546202" target="_blank"> <small>[Gizmodo]</small></a>
				<a href="https://www.engadget.com/mit-csail-smart-clothes-track-movements-160010512.html" target="_blank"> <small>[Engadget]</small></a>
				<br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/v-cdn/v-cdn.gif" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>,
				<a href="http://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>,
				<a href="http://tensorlab.cms.caltech.edu/users/anima/" target="_blank">Animashree Anandkumar</a>,
				<a href="https://homes.cs.washington.edu/~fox/" target="_blank">Dieter Fox</a>, and
				<a href="https://animesh.garg.tech/" target="_blank">Animesh Garg</a>
				<br>
				<b><font color="black">Causal Discovery in Physical Systems from Videos</font></b><br>
				<b><a href="https://nips.cc/Conferences/2020" target="_blank">NeurIPS 2020</a></b>,
				<a href="https://yunzhuli.github.io/www/V-CDN/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/abs/2007.00631" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/pairlab/v-cdn" target="_blank"> <small>[Code]</small></a>
				<a href="https://www.youtube.com/watch?v=hRsCt8xLn_8" target="_blank"> <small>[Video]</small></a>
				<a href="https://yunzhuli.github.io/www/V-CDN/poster.pdf" target="_blank"> <small>[Poster]</small></a>
				<a href="https://yunzhuli.github.io/www/V-CDN/V-CDN.bib" target="_blank"> <small>[BibTex]</small></a>
				<br>
				<small>Covered by</small>
				<a href="https://venturebeat.com/2020/07/02/ai-system-learns-to-model-how-fabrics-interact-by-watching-videos/" target="_blank"> <small>[VentureBeat]</small></a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/key_dynam/key_dynam.gif" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="http://lucasmanuelli.com/" target="_blank">Lucas Manuelli</a>,
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>,
				<a href="http://www.peteflorence.com/" target="_blank">Pete Florence</a>, and
				<a href="https://groups.csail.mit.edu/locomotion/russt.html" target="_blank">Russ Tedrake</a>
				<br>
				<b><font color="black">Keypoints into the Future: Self-Supervised Correspondence in Model-Based Reinforcement Learning</font></b><br>
				<b><a href="https://www.robot-learning.org/" target="_blank">CoRL 2020</a></b>,
				<a href="https://sites.google.com/view/keypointsintothefuture" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/abs/2009.05085" target="_blank"> <small>[Paper]</small></a>
				<a href="https://www.youtube.com/watch?v=qxC7XS4eFFw" target="_blank"> <small>[Video]</small></a>
				<a href="projects/key_dynam/key_dynam.bib" target="_blank"> <small>[BibTex]</small></a><br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/compkpm/compkpm.gif" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://people.csail.mit.edu/liyunzhu/"><b>Yunzhu Li</b></a>*,
				<a href="http://people.csail.mit.edu/hehaodele/" target="_blank">Hao He</a>*,
				<a href="http://jiajunwu.com" target="_blank">Jiajun Wu</a>,
				<a href="http://people.csail.mit.edu/dina/" target="_blank">Dina Katabi</a>, and
				<a href="https://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>
				<br>
				<b><font color="black">Learning Compositional Koopman Operators for Model-Based Control</font></b><br>
				<b><a href="https://iclr.cc/Conferences/2020" target="_blank">ICLR 2020</a></b>,
				<a href="http://koopman.csail.mit.edu/" target="_blank"> <small>[Project]</small></a>
				<a href="https://openreview.net/forum?id=H1ldzA4tPr" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/YunzhuLi/CompositionalKoopmanOperators" target="_blank"><small>[Code]</small></a>
				<a href="projects/compkpm/compkpm.bib" target="_blank"> <small>[BibTex]</small></a>
				<a href="https://youtu.be/MnXo_hjh1Q4" target="_blank"> <small>[Video]</small></a>
				<a href="http://koopman.csail.mit.edu/poster.pdf" target="_blank"> <small>[Poster]</small></a><br>
				<font color="firebrick"><b>Spotlight Presentation (Top 6.0%)</b></font><br>
				Abridged in <b>NeurIPS 2019</b> workshop on Graph Representation Learning <a href="https://grlearning.github.io/papers/" target="_blank"><small>[Link]</small></a>
				
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/stag/stag_lowres.jpg" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://people.csail.mit.edu/subras/" target="_blank">Subramanian Sundaram</a>,
				<a href="https://people.csail.mit.edu/pkellnho/" target="_blank">Petr Kellnhofer</a>,
				<a href="https://people.csail.mit.edu/liyunzhu/"><b>Yunzhu Li</b></a>,
				<a href="https://people.csail.mit.edu/junyanz/" target="_blank">Jun-Yan Zhu</a>,
				<a href="https://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>, and
				<a href="https://people.csail.mit.edu/wojciech/" target="_blank">Wojciech Matusik</a>
				<br>
				<b><font color="black">Learning the Signatures of the Human Grasp Using a Scalable Tactile Glove</font></b><br>
				<b><a href="https://www.nature.com/" target="_blank">Nature</a></b> 569, 698–702 (2019),
				<font color="firebrick"><b>5-year Impact Factor: 54.637</b></font>
				<br>
				<a href="http://stag.csail.mit.edu/" target="_blank"> <small>[Project]</small></a>
				<a href="https://www.nature.com/articles/s41586-019-1234-z" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/Erkil1452/touch" target="_blank"> <small>[Code]</small></a>
				<a href="http://stag.csail.mit.edu/files/sundaram2019stag.bib" target="_blank"> <small>[BibTex]</small></a>,

				<small>Collected by</small>
				<a href="projects/stag/stag_mit_museum.jpg" target="_blank"> <small>MIT Museum</small></a><br>

				<small>Covered by</small>
				<a href="http://news.mit.edu/2019/sensor-glove-human-grasp-robotics-0529" target="_blank"> <small>[MIT News]</small></a>
				<a href="https://www.nature.com/articles/d41586-019-01593-w" target="_blank"> <small>[Nature News & Views]</small></a>
				<a href="https://devicematerialscommunity.nature.com/users/257334-subramanian-sundaram/posts/49420-learning-dexterity-from-humans" target="_blank"> <small>[Nature communities]</small></a>
				<a href="https://www.economist.com/science-and-technology/2019/05/30/improving-robots-grasp-requires-a-new-way-to-measure-it-in-humans" target="_blank"> <small>[The Economist]</small></a>
				<a href="https://www.pbs.org/wgbh/nova/article/electronic-glove-pressure-sensors/" target="_blank"> <small>[PBS NOVA]</small></a>
				<a href="https://www.bbc.co.uk/sounds/play/p079yr9y" target="_blank"> <small>[BBC Radio]</small></a>
				<a href="https://www.newscientist.com/article/2204736-smart-glove-works-out-what-youre-holding-from-its-weight-and-shape/" target="_blank"> <small>[NewScientist]</small></a>
				</div>
			</div><hr>

      			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/dpi/dpi.png" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>,
				<a href="http://jiajunwu.com" target="_blank">Jiajun Wu</a>,
				<a href="https://groups.csail.mit.edu/locomotion/russt.html" target="_blank">Russ Tedrake</a>,
				<a href="https://web.mit.edu/cocosci/josh.html" target="_blank">Joshua B. Tenenbaum</a>, and
				<a href="http://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>
				<br>
				<b><font color="black">Learning Particle Dynamics for Manipulating Rigid Bodies, Deformable Objects, and Fluids</font></b><br>
				<b><a href="https://iclr.cc/Conferences/2019" target="_blank">ICLR 2019</a></b>,
				<a href="http://dpi.csail.mit.edu" target="_blank"> <small>[Project]</small></a>
				<a href="http://dpi.csail.mit.edu/dpi-paper.pdf" target="_blank"> <small>[PDF]</small></a>
				<a href="https://github.com/YunzhuLi/DPI-Net" target="_blank"> <small>[Code]</small></a>
				<a href="http://dpi.csail.mit.edu/dpi.bib" target="_blank"> <small>[BibTex]</small></a>
				<a href="projects/dpi/dpi-poster.pdf" target="_blank"><small>[Poster]</small></a>
				<a href="https://www.youtube.com/watch?v=FrPpP7aW3Lg" target="_blank"> <small>[Video]</small></a><br>

				<small>Covered by</small>
				<a href="http://news.mit.edu/2019/robots-object-manipulation-particle-simulator-0417" target="_blank"> <small>[MIT News]</small></a>
				<a href="https://www.engadget.com/2019/04/21/mit-particle-simulator-helps-robots-make-sushi/" target="_blank"> <small>[Engadget]</small></a>
				<a href="https://news.developer.nvidia.com/laying-the-foundation-for-better-object-manipulation-in-robotics/" target="_blank"> <small>[NVIDIA Developer]</small></a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/infogail/infogail.png" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>,
				<a href="http://tsong.me/" target="_blank">Jiaming Song</a>, and
				<a href="http://cs.stanford.edu/~ermon/" target="_blank">Stefano Ermon</a>
				<br>
				<b><font color="black">InfoGAIL: Interpretable Imitation Learning from Visual Demonstrations</font></b><br>
				<b><a href="https://nips.cc/Conferences/2017" target="_blank">NIPS 2017</a></b>,
				<a href="projects/infogail/infogail-paper.pdf" target="_blank"> <small>[PDF]</small></a>
				<a href="https://github.com/YunzhuLi/InfoGAIL" target="_blank"> <small>[Code]</small></a>
				<a href="projects/infogail/infogail.bib" target="_blank"> <small>[BibTex]</small></a>
				<a href="projects/infogail/infogail-poster.pdf" target="_blank"><small>[Poster]</small></a>
				<a href="https://www.youtube.com/watch?v=YtNPBAW6h5k" target="_blank"> <small>[Video]</small></a>
				</div>
			</div><hr>

		</script>

		
		<script id="pubs_by_date" language="text">
		  <font color="black">(* indicates equal contribution)</font><br><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/dyn-res/dyn-res.gif" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://wangyixuan12.github.io/" target="_blank">Yixuan Wang</a>*,
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>*,
				<a href="https://krdc.web.illinois.edu/" target="_blank">Katherine Driggs-Campbell</a>,
				<a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>, and
				<a href="http://jiajunwu.com" target="_blank">Jiajun Wu</a>
				<br>
				<b><font color="black">Dynamic-Resolution Model Learning for Object Pile Manipulation</font></b><br>
				<b><a href="https://roboticsconference.org/" target="_blank">RSS 2023</a></b>
				</div>
			</div><hr>

      <div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/actionosf/actionosf.jpg" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://s-tian.github.io/" target="_blank">Stephen Tian</a>*,
        <a href="">Yancheng Cai</a>*,
        <a href="https://kovenyu.com/" target="_blank">Hong-Xing Yu</a>,
				<a href="https://zakharos.github.io/" target="_blank">Sergey Zakharov</a>,
        <a href="https://www.thekatherineliu.com/" target="_blank">Katherine Liu</a>,
				<a href="https://adriengaidon.com/" target="_blank">Adrien Gaidon</a>,
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>, and
				<a href="http://jiajunwu.com" target="_blank">Jiajun Wu</a>
				<br>
				<b><font color="black">Multi-Object Manipulation via Object-Centric Neural Scattering Functions</font></b><br>
				<b><a href="https://cvpr2023.thecvf.com/" target="_blank">CVPR 2023</a></b>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/objectfolderbenchmark/objectfolderbenchmark.jpg" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://ai.stanford.edu/~rhgao/" target="_blank">Ruohan Gao</a>*,
        <a href="https://dou-yiming.github.io/" target="_blank">Yiming Dou</a>*,
        <a href="https://haolirobo.github.io/" target="_blank">Hao Li</a>*,
				<a href="https://tanmay-agarwal.com/" target="_blank">Tanmay Agarwal</a>,
        <a href="http://web.stanford.edu/~bohg/" target="_blank">Jeannette Bohg</a>,
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>,
        <a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>, and
				<a href="http://jiajunwu.com" target="_blank">Jiajun Wu</a>
				<br>
				<b><font color="black">The ObjectFolder Benchmark: Multisensory Learning with Neural and Real Objects</font></b><br>
				<b><a href="https://cvpr2023.thecvf.com/" target="_blank">CVPR 2023</a></b>,
				<a href="https://objectfolder.stanford.edu/" target="_blank"> <small>[Project]</small></a>
				<a href="https://www.objectfolder.org/swan_vis/" target="_blank"> <small>[Demo]</small></a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/dec-ssl/dec-ssl.jpg" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://www.csail.mit.edu/person/lirui-wang" target="_blank">Lirui Wang</a>,
				<a href="https://kzhang66.github.io/" target="_blank">Kaiqing Zhang</a>,
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>,
				<a href="https://scholar.google.com/citations?user=OsP7JHAAAAAJ&hl=en" target="_blank">Yonglong Tian</a>, and
				<a href="https://groups.csail.mit.edu/locomotion/russt.html" target="_blank">Russ Tedrake</a>
				<br>
				<b><font color="black">Does Learning from Decentralized Non-IID Unlabeled Data Benefit from Self Supervision?</font></b><br>
				<b><a href="https://iclr.cc/Conferences/2023" target="_blank">ICLR 2023</a></b>,
				<a href="https://arxiv.org/abs/2210.10947" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/liruiw/Dec-SSL" target="_blank"> <small>[Code]</small></a>
				<a href="https://www.youtube.com/watch?v=CbSGwsihnEk" target="_blank"> <small>[Video]</small></a>
				<a href="projects/dec-ssl/dec-ssl.bib" target="_blank"> <small>[BibTex]</small></a><br>
				</div>
			</div><hr>

		  <div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/comp_nerf_dy/combined.gif" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://dannydriess.github.io/" target="_blank">Danny Driess</a>,
				<a href="https://sites.google.com/view/zhiao-huang" target="_blank">Zhiao Huang</a>,
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>,
				<a href="https://groups.csail.mit.edu/locomotion/russt.html" target="_blank">Russ Tedrake</a>, and
				<a href="https://www.user.tu-berlin.de/mtoussai/" target="_blank">Marc Toussaint</a>
				<br>
				<b><font color="black">Learning Multi-Object Dynamics with Compositional Neural Radiance Fields</font></b><br>
				<b><a href="https://corl2022.org/" target="_blank">CoRL 2022</a></b>,
				<a href="https://dannydriess.github.io/compnerfdyn/index.html" target="_blank"> <small>[Project & Video]</small></a>
				<a href="https://openreview.net/forum?id=qUvTmyGpnm7" target="_blank"> <small>[Paper]</small></a>
				<a href="projects/comp_nerf_dy/comp_nerf_dy.bib" target="_blank"> <small>[BibTex]</small></a><br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/pasta/PASTA_bright.gif" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://xingyu-lin.github.io/" target="_blank">Xingyu Lin</a>*,
				<a href="https://www.linkedin.com/in/carlqi/" target="_blank">Carl Qi</a>*,
				<a href="https://yunchuzhang.github.io/" target="_blank">Yunchu Zhang</a>,
				<a href="https://sites.google.com/view/zhiao-huang" target="_blank">Zhiao Huang</a>,
				<a href="https://www.cs.cmu.edu/~katef/" target="_blank">Katerina Fragkiadaki</a>,
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>,
				<a href="https://people.csail.mit.edu/ganchuang/" target="_blank">Chuang Gan</a>, and
				<a href="https://davheld.github.io/" target="_blank">David Held</a>
				<br>
				<b><font color="black">Planning with Spatial-Temporal Abstraction from Point Clouds for Deformable Object Manipulation</font></b><br>
				<b><a href="https://corl2022.org/" target="_blank">CoRL 2022</a></b>,
				<a href="https://sites.google.com/view/pasta-plan" target="_blank"> <small>[Project & Video]</small></a>
				<a href="https://openreview.net/forum?id=tyxyBj2w4vw" target="_blank"> <small>[Paper]</small></a>
				<a href="projects/pasta/pasta.bib" target="_blank"> <small>[BibTex]</small></a><br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/nerf-rl/nerf-rl.gif" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://dannydriess.github.io/" target="_blank">Danny Driess</a>,
				<a href="https://ingmarschubert.com/" target="_blank">Ingmar Schubert</a>,
				<a href="https://www.peteflorence.com/" target="_blank">Pete Florence</a>,
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>, and
				<a href="https://www.user.tu-berlin.de/mtoussai/" target="_blank">Marc Toussaint</a>
				<br>
				<b><font color="black">Reinforcement Learning with Neural Radiance Fields</font></b><br>
				<b><a href="https://neurips.cc/Conferences/2022" target="_blank">NeurIPS 2022</a></b>,
				<a href="https://dannydriess.github.io/nerf-rl/index.html" target="_blank"> <small>[Project & Video]</small></a>
				<a href="https://dannydriess.github.io/papers/22-driess-NeRF-RL-preprint.pdf" target="_blank"> <small>[Paper]</small></a>
				<a href="projects/nerf-rl/nerf-rl.bib" target="_blank"> <small>[BibTex]</small></a><br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/ActionSense/ActionSense.jpg" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://www.josephdelpreto.com/" target="_blank">Joseph DelPreto</a>*,
				<a href="https://chaoliu.tech/" target="_blank">Chao Liu</a>*,
				<a href="https://yyueluo.com/" target="_blank">Yiyue Luo</a>,
        			<a href="https://www.csail.mit.edu/person/michael-foshey" target="_blank">Michael Foshey</a>,
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>,
				<a href="http://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>,
        			<a href="https://cdfg.mit.edu/wojciech" target="_blank">Wojciech Matusik</a>, and
				<a href="https://danielarus.csail.mit.edu/" target="_blank">Daniela Rus</a>
				<br>
				<b><font color="black">ActionSense: A Multimodal Dataset and Recording Framework for Human Activities Using Wearable Sensors in a Kitchen Environment</font></b><br>
				<b><a href="https://neurips.cc/Conferences/2022" target="_blank">NeurIPS 2022 Datasets and Benchmarks</a></b>,
				<a href="https://action-net.csail.mit.edu/" target="_blank"> <small>[Project & Video]</small></a>
				<a href="https://openreview.net/forum?id=olvz0gAdGOX" target="_blank"> <small>[Paper]</small></a>
				<a href="projects/ActionSense/ActionSense.bib" target="_blank"> <small>[BibTex]</small></a><br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/robocraft/robocraft.gif" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://hshi74.github.io/" target="_blank">Haochen Shi</a>*,
				<a href="http://hxu.rocks/" target="_blank">Huazhe Xu</a>*,
				<a href="https://sites.google.com/view/zhiao-huang" target="_blank">Zhiao Huang</a>,
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>, and
				<a href="http://jiajunwu.com" target="_blank">Jiajun Wu</a>
				<br>
				<b><font color="black">RoboCraft: Learning to See, Simulate, and Shape Elasto-Plastic Objects with Graph Networks</font></b><br>
				<b><a href="https://roboticsconference.org/" target="_blank">RSS 2022</a></b>,
				<a href="http://hxu.rocks/robocraft/" target="_blank"> <small>[Project & Video]</small></a>
				<a href="https://arxiv.org/abs/2205.02909" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/hshi74/RoboCraft" target="_blank"> <small>[Code]</small></a>
				<a href="projects/robocraft/robocraft.bib" target="_blank"> <small>[BibTex]</small></a><br>
				Abridged in <b>ICRA 2022</b> workshop on Representing and Manipulating Deformable Objects <a href="https://deformable-workshop.github.io/icra2022/" target="_blank"><small>[Link]</small></a><br>
				<small>Covered by</small>
				<a href="https://news.mit.edu/2022/robots-play-play-dough-0623" target="_blank"> <small>[MIT News]</small></a>
				<a href="https://www.newscientist.com/article/2325970-ai-powered-robot-learned-to-make-letters-out-of-play-doh-on-its-own/" target="_blank"> <small>[NewScientist]</small></a>
				<a href="https://techcrunch.com/2022/06/23/a-quick-trip-to-mars/" target="_blank"> <small>[TechCrunch]</small></a>
				<a href="https://hai.stanford.edu/news/training-robot-shape-letters-play-doh" target="_blank"> <small>[Stanford HAI]</small></a>

				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/TRO_multiobj_manip/TRO_multiobj_manip.jpg" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://scholar.google.com/citations?user=0KfKHOsAAAAJ&hl=en" target="_blank">Zherong Pan</a>,
				<a href="https://andyzeng.github.io/" target="_blank">Andy Zeng</a>,
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>,
				<a href="https://arc-l.github.io/" target="_blank">Jingjin Yu</a>, and
				<a href="https://kkhauser.web.illinois.edu/" target="_blank">Kris Hauser</a>
				<br>
				<b><font color="black">Algorithms and Systems for Manipulating Multiple Objects</font></b><br>
				<b><a href="https://www.ieee-ras.org/publications/t-ro" target="_blank">T-RO 2022</a></b>,
				<a href="https://ieeexplore.ieee.org/abstract/document/9893496" target="_blank"> <small>[Paper]</small></a>
				<a href="projects/TRO_multiobj_manip/TRO_multiobj_manip.bib" target="_blank"> <small>[BibTex]</small></a><br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/diffskill/DiffSkill-compressed.gif" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://xingyu-lin.github.io/" target="_blank">Xingyu Lin</a>,
				<a href="https://sites.google.com/view/zhiao-huang" target="_blank">Zhiao Huang</a>,
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>,
				<a href="https://web.mit.edu/cocosci/josh.html" target="_blank">Joshua B. Tenenbaum</a>,
				<a href="https://davheld.github.io/" target="_blank">David Held</a>, and
				<a href="http://people.csail.mit.edu/ganchuang/" target="_blank">Chuang Gan</a>
				<br>
				<b><font color="black">DiffSkill: Skill Abstraction from Differentiable Physics for Deformable Object Manipulations with Tools</font></b><br>
				<b><a href="https://iclr.cc/Conferences/2022" target="_blank">ICLR 2022</a></b>,
				<a href="https://xingyu-lin.github.io/diffskill/" target="_blank"> <small>[Project]</small></a>
				<a href="https://openreview.net/forum?id=Kef8cKdHWpP" target="_blank"> <small>[Paper]</small></a>
				<a href="projects/diffskill/DiffSkill.bib" target="_blank"> <small>[BibTex]</small></a>
				<br>
				<small>Covered by</small>
				<a href="https://news.mit.edu/2022/robotic-deformable-object-0331" target="_blank"> <small>[MIT News]</small></a>
				<a href="https://techcrunch.com/2022/03/31/better-learning-through-complex-dough-manipulation/" target="_blank"> <small>[TechCrunch]</small></a>
				<a href="https://bdtechtalks.com/2022/05/09/diffskill-robotics-deformable-object-manipulation/" target="_blank"> <small>[TechTalks]</small></a>
				<br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/comphy/ComPhy.jpg" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://zfchenunique.github.io/" target="_blank">Zhenfang Chen</a>,
				<a href="https://scholar.google.com/citations?user=SwxS_JkAAAAJ&hl=en" target="_blank">Kexin Yi</a>,
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>,
				<a href="https://dingmyu.github.io/" target="_blank">Mingyu Ding</a>,
				<a href="http://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>,
				<a href="https://web.mit.edu/cocosci/josh.html" target="_blank">Joshua B. Tenenbaum</a>, and
				<a href="http://people.csail.mit.edu/ganchuang/" target="_blank">Chuang Gan</a>
				<br>
				<b><font color="black">ComPhy: Compositional Physical Reasoning of Objects and Events from Videos</font></b><br>
				<b><a href="https://iclr.cc/Conferences/2022" target="_blank">ICLR 2022</a></b>,
				<a href="https://comphyreasoning.github.io/" target="_blank"> <small>[Project]</small></a>
				<a href="https://openreview.net/forum?id=PgNEYaIc81Q" target="_blank"> <small>[Paper]</small></a>
				<a href="projects/comphy/ComPhy.bib" target="_blank"> <small>[BibTex]</small></a><br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/DAIS/DAIS.png" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://www.csail.mit.edu/person/lujie-yang" target="_blank">Lujie Yang</a>,
				<a href="https://kzhang66.github.io/" target="_blank">Kaiqing Zhang</a>,
				<a href="https://alexandreamice.github.io/" target="_blank">Alexandre Amice</a>,
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>, and
				<a href="https://groups.csail.mit.edu/locomotion/russt.html" target="_blank">Russ Tedrake</a>
				<br>
				<b><font color="black">Discrete Approximate Information States in Partially Observable Environments</font></b><br>
				<b><a href="https://acc2022.a2c2.org/" target="_blank">ACC 2022</a></b>,
				<a href="projects/DAIS/DAIS.pdf" target="_blank"> <small>[Paper]</small></a><br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/nerf-dy/nerf-dy-multiview.gif" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>*,
				<a href="https://people.csail.mit.edu/lishuang/" target="_blank">Shuang Li</a>*,
				<a href="https://vsitzmann.github.io/" target="_blank">Vincent Sitzmann</a>,
				<a href="https://people.csail.mit.edu/pulkitag/" target="_blank">Pulkit Agrawal</a>, and
				<a href="http://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>
				<br>
				<b><font color="black">3D Neural Scene Representations for Visuomotor Control</font></b><br>
				<b><a href="https://www.robot-learning.org/" target="_blank">CoRL 2021</a></b>,
				<a href="https://3d-representation-learning.github.io/nerf-dy/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/abs/2107.04004" target="_blank"> <small>[Paper]</small></a>
				<a href="https://youtu.be/ELPMiifELGc" target="_blank"> <small>[Video]</small></a>
				<a href="https://openreview.net/forum?id=zv3NYgRZ7Qo" target="_blank"> <small>[OpenReview]</small></a>
				<a href="https://3d-representation-learning.github.io/nerf-dy/nerf-dy.bib" target="_blank"> <small>[BibTex]</small></a><br>
				<font color="firebrick"><b>Oral Presentation (Top 6.5%)</b></font><br>
				Abridged in <b>RSS 2021</b> workshop on Visual Learning and Reasoning for Robotics <a href="https://rssvlrr.github.io/" target="_blank"><small>[Link]</small></a><br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/senstextile/senstextile.jpg" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
        			<a href="https://yyueluo.com/" target="_blank">Yiyue Luo</a>,
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>,
        			<a href="https://pratyushasharma.github.io/" target="_blank">Pratyusha Sharma</a>,
        			<a href="https://www.csail.mit.edu/person/wan-shou" target="_blank">Wan Shou</a>,
        			<a href="http://people.csail.mit.edu/kuiwu" target="_blank">Kui Wu</a>,
        			<a href="https://www.csail.mit.edu/person/michael-foshey" target="_blank">Michael Foshey</a>,
        			<a href="https://www.csail.mit.edu/person/beichen-li" target="_blank">Beichen Li</a>,
        			<a href="http://www-mtl.mit.edu/wpmu/tpalacios/" target="_blank">Tomas Palacios</a>,
        			<a href="https://groups.csail.mit.edu/vision/torralbalab/" target="_blank">Antonio Torralba</a>, and
        			<a href="https://cdfg.mit.edu/wojciech" target="_blank">Wojciech Matusik</a>
				<br>
				<b><font color="black">Learning Human-environment Interactions using Conformal Tactile Textiles</font></b><br>
				<b><a href="https://www.nature.com/natelectron/" target="_blank">Nature Electronics</a></b> 4, 193–201 (2021),
				<font color="firebrick"><b>5-year Impact Factor: 33.695</b></font>
				<br>
				<a href="http://senstextile.csail.mit.edu/" target="_blank"> <small>[Project]</small></a>
				<a href="https://www.nature.com/articles/s41928-021-00558-0" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/YunzhuLi/senstextile" target="_blank"> <small>[Code]</small></a>
				<a href="projects/senstextile/senstextile.bib" target="_blank"> <small>[BibTex]</small></a>
				<br>
				<small>Featured on the</small>
				<a href="https://www.nature.com/natelectron/volumes/4/issues/3" target="_blank"> <small>cover</small></a>
				<small>of the issue.</small>
				<small>Editorial comments</small>
				<a href="https://www.nature.com/articles/s41928-021-00567-z" target="_blank"> <small>[Link]</small></a>
				<br>
				<small>Covered by</small>
				<a href="https://www.nature.com/articles/s41928-021-00560-6" target="_blank"> <small>[Nature Electronics News & Views]</small></a>
				<a href="https://www.csail.mit.edu/news/smart-clothes-can-measure-your-movements" target="_blank"> <small>[MIT CSAIL News]</small></a>
				<a href="https://gizmodo.com/researchers-might-have-finally-cracked-smart-clothing-1846546202" target="_blank"> <small>[Gizmodo]</small></a>
				<a href="https://www.engadget.com/mit-csail-smart-clothes-track-movements-160010512.html" target="_blank"> <small>[Engadget]</small></a>
				<br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/phystouch/phystouch.jpeg" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
					<a href="https://github.com/sjtuzq" target="_blank">Qiang Zhang</a>*,
					<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>*,
					<a href="https://yyueluo.com/" target="_blank">Yiyue Luo</a>,
					<a href="https://showone90.wixsite.com/show" target="_blank">Wan Shou</a>,
					<a href="https://www.csail.mit.edu/person/michael-foshey" target="_blank">Michael Foshey</a>,
					<a href="https://thinklab.sjtu.edu.cn/" target="_blank">Junchi Yan</a>,
					<a href="https://scholar.google.com/citations?user=rRJ9wTJMUB8C&hl=en" target="_blank">Joshua B. Tenenbaum</a>,
					<a href="https://cdfg.mit.edu/wojciech" target="_blank">Wojciech Matusik</a>, and
					<a href="http://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>
					<br>
					<b><font color="black">Dynamic Modeling of Hand-Object Interactions via Tactile Sensing</font></b><br>
					<b><a href="https://www.iros2021.org/" target="_blank">IROS 2021</a></b>,
					<a href="http://phystouch.csail.mit.edu/" target="_blank"> <small>[Project]</small></a>
					<a href="https://arxiv.org/abs/2109.04378" target="_blank"> <small>[Paper]</small></a>
					<a href="https://www.youtube.com/watch?v=rBN5kNOw5Y8" target="_blank"> <small>[Video]</small></a>
					<a href="projects/phystouch/phystouch.bib" target="_blank"> <small>[BibTex]</small></a>
					<br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/intcarpet/logo.jpg" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
        			<a href="https://yyueluo.com/" target="_blank">Yiyue Luo</a>,
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>,
        			<a href="https://www.csail.mit.edu/person/michael-foshey" target="_blank">Michael Foshey</a>,
        			<a href="https://www.csail.mit.edu/person/wan-shou" target="_blank">Wan Shou</a>,
        			<a href="https://pratyushasharma.github.io/" target="_blank">Pratyusha Sharma</a>,
        			<a href="http://www-mtl.mit.edu/wpmu/tpalacios/" target="_blank">Tomas Palacios</a>,
        			<a href="https://groups.csail.mit.edu/vision/torralbalab/" target="_blank">Antonio Torralba</a>, and
        			<a href="https://cdfg.mit.edu/wojciech" target="_blank">Wojciech Matusik</a>
				<br>
				<b><font color="black">Intelligent Carpet: Inferring 3D Human Pose from Tactile Signals</font></b><br>
				<b><a href="http://cvpr2021.thecvf.com/" target="_blank">CVPR 2021</a></b>,
				<a href="http://intcarpet.csail.mit.edu/" target="_blank"> <small>[Project]</small></a>
				<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Luo_Intelligent_Carpet_Inferring_3D_Human_Pose_From_Tactile_Signals_CVPR_2021_paper.pdf" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/yiyueluo/IntelligentCarpet" target="_blank"> <small>[Code]</small></a>
				<a href="https://www.youtube.com/watch?v=U6svj37h2U4" target="_blank"> <small>[Video]</small></a>
				<a href="projects/intcarpet/intcarpet.bib" target="_blank"> <small>[BibTex]</small></a>
				<br>
				<small>Covered by</small>
				<a href="https://news.mit.edu/2021/intelligent-carpet-gives-insight-human-poses-0624" target="_blank"> <small>[MIT News]</small></a>
				<a href="https://www.fastcompany.com/90648670/this-magic-carpet-can-track-your-workout" target="_blank"> <small>[Fast Company]</small></a>
				<br>
				</div>
			</div><hr>
			
			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/v-cdn/v-cdn.gif" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>,
				<a href="http://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>,
				<a href="http://tensorlab.cms.caltech.edu/users/anima/" target="_blank">Animashree Anandkumar</a>,
				<a href="https://homes.cs.washington.edu/~fox/" target="_blank">Dieter Fox</a>, and
				<a href="https://animesh.garg.tech/" target="_blank">Animesh Garg</a>
				<br>
				<b><font color="black">Causal Discovery in Physical Systems from Videos</font></b><br>
				<b><a href="https://nips.cc/Conferences/2020" target="_blank">NeurIPS 2020</a></b>,
				<a href="https://yunzhuli.github.io/www/V-CDN/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/abs/2007.00631" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/pairlab/v-cdn" target="_blank"> <small>[Code]</small></a>
				<a href="https://www.youtube.com/watch?v=hRsCt8xLn_8" target="_blank"> <small>[Video]</small></a>
				<a href="https://yunzhuli.github.io/www/V-CDN/poster.pdf" target="_blank"> <small>[Poster]</small></a>
				<a href="https://yunzhuli.github.io/www/V-CDN/V-CDN.bib" target="_blank"> <small>[BibTex]</small></a>
			  <br>	
				<small>Covered by</small>
				<a href="https://venturebeat.com/2020/07/02/ai-system-learns-to-model-how-fabrics-interact-by-watching-videos/" target="_blank"> <small>[VentureBeat]</small></a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/physical_scene_graphs/physical_scene_graphs.jpg" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://scholar.google.com/citations?user=uYbkEzYAAAAJ&hl=en" target="_blank">Daniel M. Bear</a>,
				<a href="https://scholar.google.com/citations?user=YM4x068AAAAJ&hl=en" target="_blank">Chaofei Fan</a>,
				<a href="https://scholar.google.com/citations?user=GADXPDcAAAAJ&hl=en" target="_blank">Damian Mrowca</a>,
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>,
				<a href="https://bcs.mit.edu/users/altersmitedu" target="_blank">Seth Alter</a>,
				<a href="https://sites.google.com/site/anayebihomepage/" target="_blank">Aran Nayebi</a>,
				<a href="https://bcs-r1.mit.edu/users/jeremyesmitedu" target="_blank">Jeremy Schwartz</a>,
				<a href="http://vision.stanford.edu/people.html" target="_blank">Li Fei-Fei</a>,
				<a href="http://jiajunwu.com" target="_blank">Jiajun Wu</a>,
				<a href="https://web.mit.edu/cocosci/josh.html" target="_blank">Joshua B. Tenenbaum</a>, and
				<a href="http://stanford.edu/~yamins/" target="_blank">Daniel L. K. Yamins</a>
				<br>
				<b><font color="black">Learning Physical Graph Representations from Visual Scenes</font></b><br>
				<b><a href="https://nips.cc/Conferences/2020" target="_blank">NeurIPS 2020</a></b>,
				<a href="https://neuroailab.github.io/physical-scene-graphs/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/abs/2006.12373" target="_blank"> <small>[Paper]</small></a>
				<a href="projects/physical_scene_graphs/physical_scene_graphs.bib" target="_blank"> <small>[BibTex]</small></a><br>
				<font color="firebrick"><b>Oral Presentation (Top 1.1%)</b></font><br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/key_dynam/key_dynam.gif" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="http://lucasmanuelli.com/" target="_blank">Lucas Manuelli</a>,
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>,
				<a href="http://www.peteflorence.com/" target="_blank">Pete Florence</a>, and
				<a href="https://groups.csail.mit.edu/locomotion/russt.html" target="_blank">Russ Tedrake</a>
				<br>
				<b><font color="black">Keypoints into the Future: Self-Supervised Correspondence in Model-Based Reinforcement Learning</font></b><br>
				<b><a href="https://www.robot-learning.org/" target="_blank">CoRL 2020</a></b>,
				<a href="https://sites.google.com/view/keypointsintothefuture" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/abs/2009.05085" target="_blank"> <small>[Paper]</small></a>
				<a href="https://www.youtube.com/watch?v=qxC7XS4eFFw" target="_blank"> <small>[Video]</small></a>
				<a href="projects/key_dynam/key_dynam.bib" target="_blank"> <small>[BibTex]</small></a><br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/visual_grounding/visual_grounding.gif" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>,
				<a href="https://scholar.google.com/citations?user=Rxb7o6IAAAAJ&hl=en" target="_blank">Toru Lin</a>*,
				<a href="https://scholar.google.com/citations?user=SwxS_JkAAAAJ&hl=en" target="_blank">Kexin Yi</a>*,
				<a href="https://scholar.google.com/citations?user=uYbkEzYAAAAJ&hl=en" target="_blank">Daniel M. Bear</a>,
				<a href="http://stanford.edu/~yamins/" target="_blank">Daniel L. K. Yamins</a>,
				<a href="http://jiajunwu.com" target="_blank">Jiajun Wu</a>,
				<a href="https://web.mit.edu/cocosci/josh.html" target="_blank">Joshua B. Tenenbaum</a>, and
				<a href="http://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>
				<br>
				<b><font color="black">Visual Grounding of Learned Physical Models</font></b><br>
				<b><a href="https://icml.cc/Conferences/2020" target="_blank">ICML 2020</a></b>,
				<a href="http://visual-physics-grounding.csail.mit.edu/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/abs/2004.13664" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/YunzhuLi/VGPL" target="_blank"><small>[Code]</small></a>
				<a href="projects/visual_grounding/vgpl.bib" target="_blank"> <small>[BibTex]</small></a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/compkpm/compkpm.gif" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://people.csail.mit.edu/liyunzhu/"><b>Yunzhu Li</b></a>*,
				<a href="http://people.csail.mit.edu/hehaodele/" target="_blank">Hao He</a>*,
				<a href="http://jiajunwu.com" target="_blank">Jiajun Wu</a>,
				<a href="http://people.csail.mit.edu/dina/" target="_blank">Dina Katabi</a>, and
				<a href="https://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>
				<br>
				<b><font color="black">Learning Compositional Koopman Operators for Model-Based Control</font></b><br>
				<b><a href="https://iclr.cc/Conferences/2020" target="_blank">ICLR 2020</a></b>,
				<a href="http://koopman.csail.mit.edu/" target="_blank"> <small>[Project]</small></a>
				<a href="https://openreview.net/forum?id=H1ldzA4tPr" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/YunzhuLi/CompositionalKoopmanOperators" target="_blank"><small>[Code]</small></a>
				<a href="projects/compkpm/compkpm.bib" target="_blank"> <small>[BibTex]</small></a>
				<a href="https://youtu.be/MnXo_hjh1Q4" target="_blank"> <small>[Video]</small></a>
				<a href="http://koopman.csail.mit.edu/poster.pdf" target="_blank"> <small>[Poster]</small></a><br>
				<font color="firebrick"><b>Spotlight Presentation (Top 6.0%)</b></font><br>
				Abridged in <b>NeurIPS 2019</b> workshop on Graph Representation Learning <a href="https://grlearning.github.io/papers/" target="_blank"><small>[Link]</small></a>
				
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/clevrer/clevrer.gif" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://scholar.google.com/citations?user=SwxS_JkAAAAJ&hl=en" target="_blank">Kexin Yi</a>*,
				<a href="http://people.csail.mit.edu/ganchuang/" target="_blank">Chuang Gan</a>*,
				<a href="https://people.csail.mit.edu/liyunzhu/"><b>Yunzhu Li</b></a>,
				<a href="https://sites.google.com/site/pushmeet/" target="_blank">Pushmeet Kohli</a>,
				<a href="http://jiajunwu.com" target="_blank">Jiajun Wu</a>,
				<a href="https://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>, and
				<a href="https://web.mit.edu/cocosci/josh.html" target="_blank">Joshua B. Tenenbaum</a>
				<br>
				<b><font color="black">CLEVRER: Collision Events for Video Representation and Reasoning</font></b><br>
				<b><a href="https://iclr.cc/Conferences/2020" target="_blank">ICLR 2020</a></b>,
				<a href="http://clevrer.csail.mit.edu/" target="_blank"> <small>[Project]</small></a>
				<a href="https://openreview.net/forum?id=HkxYzANYDB" target="_blank"> <small>[Paper]</small></a>
				<a href="projects/clevrer/clevrer.bib" target="_blank"> <small>[BibTex]</small></a><br>
				<font color="firebrick"><b>Spotlight Presentation (Top 6.0%)</b></font><br>

				<small>Covered by</small>
				<a href="https://www.youtube.com/watch?v=bVXPnP8k6yo" target="_blank"> <small>[Two Minute Papers]</small></a>
				<a href="https://www.technologyreview.com/2020/03/06/905479/ai-neuro-symbolic-system-reasons-like-child-deepmind-ibm-mit/" target="_blank"> <small>[MIT Technology Review]</small></a>
				<a href="https://www.wired.com/story/ai-smart-cant-grasp-cause-effect/" target="_blank"> <small>[WIRED]</small></a>
				<a href="https://venturebeat.com/2020/02/26/researchers-apply-developmental-psychology-to-ai-model-that-predicts-object-relationships/" target="_blank"> <small>[VentureBeat]</small></a>
				
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/stag/stag_lowres.jpg" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://people.csail.mit.edu/subras/" target="_blank">Subramanian Sundaram</a>,
				<a href="https://people.csail.mit.edu/pkellnho/" target="_blank">Petr Kellnhofer</a>,
				<a href="https://people.csail.mit.edu/liyunzhu/"><b>Yunzhu Li</b></a>,
				<a href="https://people.csail.mit.edu/junyanz/" target="_blank">Jun-Yan Zhu</a>,
				<a href="https://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>, and
				<a href="https://people.csail.mit.edu/wojciech/" target="_blank">Wojciech Matusik</a>
				<br>
				<b><font color="black">Learning the Signatures of the Human Grasp Using a Scalable Tactile Glove</font></b><br>
				<b><a href="https://www.nature.com/" target="_blank">Nature</a></b> 569, 698–702 (2019),
				<font color="firebrick"><b>5-year Impact Factor: 54.637</b></font>
				<br>
				<a href="http://stag.csail.mit.edu/" target="_blank"> <small>[Project]</small></a>
				<a href="https://www.nature.com/articles/s41586-019-1234-z" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/Erkil1452/touch" target="_blank"> <small>[Code]</small></a>
				<a href="http://stag.csail.mit.edu/files/sundaram2019stag.bib" target="_blank"> <small>[BibTex]</small></a>,

				<small>Collected by</small>
				<a href="projects/stag/stag_mit_museum.jpg" target="_blank"> <small>MIT Museum</small></a><br>

				<small>Covered by</small>
				<a href="http://news.mit.edu/2019/sensor-glove-human-grasp-robotics-0529" target="_blank"> <small>[MIT News]</small></a>
				<a href="https://www.nature.com/articles/d41586-019-01593-w" target="_blank"> <small>[Nature News & Views]</small></a>
				<a href="https://devicematerialscommunity.nature.com/users/257334-subramanian-sundaram/posts/49420-learning-dexterity-from-humans" target="_blank"> <small>[Nature communities]</small></a>
				<a href="https://www.economist.com/science-and-technology/2019/05/30/improving-robots-grasp-requires-a-new-way-to-measure-it-in-humans" target="_blank"> <small>[The Economist]</small></a>
				<a href="https://www.pbs.org/wgbh/nova/article/electronic-glove-pressure-sensors/" target="_blank"> <small>[PBS NOVA]</small></a>
				<a href="https://www.bbc.co.uk/sounds/play/p079yr9y" target="_blank"> <small>[BBC Radio]</small></a>
				<a href="https://www.newscientist.com/article/2204736-smart-glove-works-out-what-youre-holding-from-its-weight-and-shape/" target="_blank"> <small>[NewScientist]</small></a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/visgel/visgel.jpg" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>,
				<a href="http://people.csail.mit.edu/junyanz/" target="_blank">Jun-Yan Zhu</a>,
				<a href="https://groups.csail.mit.edu/locomotion/russt.html" target="_blank">Russ Tedrake</a>, and
				<a href="http://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>
				<br>
				<b><font color="black">Connecting Touch and Vision via Cross-Modal Prediction</font></b><br>
				<b><a href="http://cvpr2019.thecvf.com/" target="_blank">CVPR 2019</a></b>,
				<a href="http://visgel.csail.mit.edu" target="_blank"> <small>[Project]</small></a>
				<a href="http://visgel.csail.mit.edu/visgel-paper.pdf" target="_blank"> <small>[PDF]</small></a>
				<a href="https://github.com/YunzhuLi/VisGel" target="_blank"><small>[Code]</small></a>
				<a href="http://visgel.csail.mit.edu/visgel.bib" target="_blank"> <small>[BibTex]</small></a><br>

				<small>Covered by</small>
				<a href="http://news.mit.edu/2019/teaching-ai-to-connect-senses-vision-touch-0617" target="_blank"> <small>[MIT News]</small></a>
				<a href="https://www.bbc.com/news/av/technology-48711479/robot-taught-to-feel-objects-by-sight-and-other-news" target="_blank"> <small>[BBC]</small></a>
				<a href="https://www.cnn.com/2019/06/17/us/mit-robot-vision-touch-trnd/index.html" target="_blank"> <small>[CNN]</small></a>
				<a href="https://www.forbes.com/sites/charlestowersclark/2019/06/17/one-step-closer-to-human-intelligence-mit-csail-combine-sight-and-touch-in-ai/#3496256578b6" target="_blank"> <small>[Forbes]</small></a>
				<a href="https://techcrunch.com/2019/06/17/mit-develops-a-system-to-give-robots-more-human-senses/" target="_blank"> <small>[TechCrunch]</small></a>
				<a href="https://www.fastcompany.com/90365007/a-new-robot-can-now-identify-objects-by-touch" target="_blank"> <small>[Fast Company]</small></a>
				<a href="https://www.engadget.com/2019/06/17/robot-identify-sight-touch/" target="_blank"> <small>[Engadget]</small></a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/dpi/dpi.png" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>,
				<a href="http://jiajunwu.com" target="_blank">Jiajun Wu</a>,
				<a href="https://groups.csail.mit.edu/locomotion/russt.html" target="_blank">Russ Tedrake</a>,
				<a href="https://web.mit.edu/cocosci/josh.html" target="_blank">Joshua B. Tenenbaum</a>, and
				<a href="http://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>
				<br>
				<b><font color="black">Learning Particle Dynamics for Manipulating Rigid Bodies, Deformable Objects, and Fluids</font></b><br>
				<b><a href="https://iclr.cc/Conferences/2019" target="_blank">ICLR 2019</a></b>,
				<a href="http://dpi.csail.mit.edu" target="_blank"> <small>[Project]</small></a>
				<a href="http://dpi.csail.mit.edu/dpi-paper.pdf" target="_blank"> <small>[PDF]</small></a>
				<a href="https://github.com/YunzhuLi/DPI-Net" target="_blank"> <small>[Code]</small></a>
				<a href="http://dpi.csail.mit.edu/dpi.bib" target="_blank"> <small>[BibTex]</small></a>
				<a href="projects/dpi/dpi-poster.pdf" target="_blank"><small>[Poster]</small></a>
				<a href="https://www.youtube.com/watch?v=FrPpP7aW3Lg" target="_blank"> <small>[Video]</small></a><br>

				<small>Covered by</small>
				<a href="http://news.mit.edu/2019/robots-object-manipulation-particle-simulator-0417" target="_blank"> <small>[MIT News]</small></a>
				<a href="https://www.engadget.com/2019/04/21/mit-particle-simulator-helps-robots-make-sushi/" target="_blank"> <small>[Engadget]</small></a>
				<a href="https://news.developer.nvidia.com/laying-the-foundation-for-better-object-manipulation-in-robotics/" target="_blank"> <small>[NVIDIA Developer]</small></a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/propnet/propnet-1.png" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>,
				<a href="http://jiajunwu.com" target="_blank">Jiajun Wu</a>,
				<a href="http://people.csail.mit.edu/junyanz/" target="_blank">Jun-Yan Zhu</a>,
				<a href="https://web.mit.edu/cocosci/josh.html" target="_blank">Joshua B. Tenenbaum</a>,
				<a href="http://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>, and
				<a href="https://groups.csail.mit.edu/locomotion/russt.html" target="_blank">Russ Tedrake</a>
				<br>
				<b><font color="black">Propagation Networks for Model-Based Control Under Partial Observation</font></b><br>
				<b><a href="https://www.icra2019.org/" target="_blank">ICRA 2019</a></b>,
				<a href="http://propnet.csail.mit.edu" target="_blank"> <small>[Project]</small></a>
				<a href="http://propnet.csail.mit.edu/propnet-paper.pdf" target="_blank"> <small>[PDF]</small></a>
				<a href="https://github.com/YunzhuLi/PropNet" target="_blank"><small>[Code]</small></a>
				<a href="http://propnet.csail.mit.edu/propnet.bib" target="_blank"> <small>[BibTex]</small></a>
				<a href="https://www.youtube.com/watch?v=ZAxHXegkz48" target="_blank"> <small>[Video]</small></a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/infogail/infogail.png" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>,
				<a href="http://tsong.me/" target="_blank">Jiaming Song</a>, and
				<a href="http://cs.stanford.edu/~ermon/" target="_blank">Stefano Ermon</a>
				<br>
				<b><font color="black">InfoGAIL: Interpretable Imitation Learning from Visual Demonstrations</font></b><br>
				<b><a href="https://nips.cc/Conferences/2017" target="_blank">NIPS 2017</a></b>,
				<a href="projects/infogail/infogail-paper.pdf" target="_blank"> <small>[PDF]</small></a>
				<a href="https://github.com/YunzhuLi/InfoGAIL" target="_blank"> <small>[Code]</small></a>
				<a href="projects/infogail/infogail.bib" target="_blank"> <small>[BibTex]</small></a>
				<a href="projects/infogail/infogail-poster.pdf" target="_blank"><small>[Poster]</small></a>
				<a href="https://www.youtube.com/watch?v=YtNPBAW6h5k" target="_blank"> <small>[Video]</small></a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/skin/detection_tracking.png" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>*,
				<a href="http://cs.stanford.edu/people/esteva/home/index.html" target="_blank">Andre Esteva</a>*,
				<a href="https://stanford.edu/~kuprel/" target="_blank">Brett Kuprel</a>,
				<a href="https://profiles.stanford.edu/roberto-novoa" target="_blank">Rob Novoa</a>,
				<a href="https://profiles.stanford.edu/justin-ko" target="_blank">Justin Ko</a>, and
				<a href="http://robots.stanford.edu/" target="_blank">Sebastian Thrun</a>
				<br>
				<b><font color="black">Skin Cancer Detection and Tracking using Data Synthesis and Deep Learning</font></b><br>
				<a href="https://nips.cc/Conferences/2016" target="_blank"><b>NIPS 2016</b> Workshop on Machine Learning for Health</a><br>
				<a href="http://w3phiai2017.w3phi.com/" target="_blank"><b>AAAI 2017</b> Joint Workshop on Health Intelligence</a><br>
				<a href="projects/skin/detection_tracking-paper.pdf" target="_blank"> <small>[PDF]</small></a>
				<a href="projects/skin/skin.bib" target="_blank"> <small>[BibTex]</small></a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/conv3d/face_detection_conv3d.png" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>*,
				Benyuan Sun*,
				<a href="http://www.stat.ucla.edu/~tfwu/" target="_blank">Tianfu Wu</a>, and
				<a href="http://www.idm.pku.edu.cn/staff/wangyizhou/" target="_blank">Yizhou Wang</a>
				<br>
				<b><font color="black">Face Detection with End-to-End Integration of a ConvNet and a 3D Model</font></b><br>
				<b><a href="http://www.eccv2016.org/" target="_blank">ECCV 2016</a></b>,
				<a href="projects/conv3d/face_detection_conv3d-paper.pdf" target="_blank"> <small>[PDF]</small></a>
				<a href="https://github.com/tfwu/FaceDetection-ConvNet-3D" target="_blank"> <small>[Code]</small></a>
				<a href="projects/conv3d/conv3d.bib" target="_blank"> <small>[BibTex]</small></a>
				<a href="projects/conv3d/face_detection_conv3d-poster.pdf" target="_blank"> <small>[Poster]</small></a>
				</div>
			</div><hr>

		</script>


		<script id="pubs_by_topic" language="text">
		
			<font color="black">(* indicates equal contribution)</font><br><hr>

			<div id="dynam" style="padding-top: 80px; margin-top: -80px;">
			  <h5>Robotic Manipulation</h5>
			</div><br>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/dyn-res/dyn-res.gif" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://wangyixuan12.github.io/" target="_blank">Yixuan Wang</a>*,
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>*,
				<a href="https://krdc.web.illinois.edu/" target="_blank">Katherine Driggs-Campbell</a>,
				<a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>, and
				<a href="http://jiajunwu.com" target="_blank">Jiajun Wu</a>
				<br>
				<b><font color="black">Dynamic-Resolution Model Learning for Object Pile Manipulation</font></b><br>
				<b><a href="https://roboticsconference.org/" target="_blank">RSS 2023</a></b>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/actionosf/actionosf.jpg" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://s-tian.github.io/" target="_blank">Stephen Tian</a>*,
        <a href="">Yancheng Cai</a>*,
        <a href="https://kovenyu.com/" target="_blank">Hong-Xing Yu</a>,
				<a href="https://zakharos.github.io/" target="_blank">Sergey Zakharov</a>,
        <a href="https://www.thekatherineliu.com/" target="_blank">Katherine Liu</a>,
				<a href="https://adriengaidon.com/" target="_blank">Adrien Gaidon</a>,
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>, and
				<a href="http://jiajunwu.com" target="_blank">Jiajun Wu</a>
				<br>
				<b><font color="black">Multi-Object Manipulation via Object-Centric Neural Scattering Functions</font></b><br>
				<b><a href="https://cvpr2023.thecvf.com/" target="_blank">CVPR 2023</a></b>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/comp_nerf_dy/combined.gif" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://dannydriess.github.io/" target="_blank">Danny Driess</a>,
				<a href="https://sites.google.com/view/zhiao-huang" target="_blank">Zhiao Huang</a>,
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>,
				<a href="https://groups.csail.mit.edu/locomotion/russt.html" target="_blank">Russ Tedrake</a>, and
				<a href="https://www.user.tu-berlin.de/mtoussai/" target="_blank">Marc Toussaint</a>
				<br>
				<b><font color="black">Learning Multi-Object Dynamics with Compositional Neural Radiance Fields</font></b><br>
				<b><a href="https://corl2022.org/" target="_blank">CoRL 2022</a></b>,
				<a href="https://dannydriess.github.io/compnerfdyn/index.html" target="_blank"> <small>[Project & Video]</small></a>
				<a href="https://openreview.net/forum?id=qUvTmyGpnm7" target="_blank"> <small>[Paper]</small></a>
				<a href="projects/comp_nerf_dy/comp_nerf_dy.bib" target="_blank"> <small>[BibTex]</small></a><br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/pasta/PASTA_bright.gif" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://xingyu-lin.github.io/" target="_blank">Xingyu Lin</a>*,
				<a href="https://www.linkedin.com/in/carlqi/" target="_blank">Carl Qi</a>*,
				<a href="https://yunchuzhang.github.io/" target="_blank">Yunchu Zhang</a>,
				<a href="https://sites.google.com/view/zhiao-huang" target="_blank">Zhiao Huang</a>,
				<a href="https://www.cs.cmu.edu/~katef/" target="_blank">Katerina Fragkiadaki</a>,
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>,
				<a href="https://people.csail.mit.edu/ganchuang/" target="_blank">Chuang Gan</a>, and
				<a href="https://davheld.github.io/" target="_blank">David Held</a>
				<br>
				<b><font color="black">Planning with Spatial-Temporal Abstraction from Point Clouds for Deformable Object Manipulation</font></b><br>
				<b><a href="https://corl2022.org/" target="_blank">CoRL 2022</a></b>,
				<a href="https://sites.google.com/view/pasta-plan" target="_blank"> <small>[Project & Video]</small></a>
				<a href="https://openreview.net/forum?id=tyxyBj2w4vw" target="_blank"> <small>[Paper]</small></a>
				<a href="projects/pasta/pasta.bib" target="_blank"> <small>[BibTex]</small></a><br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/nerf-rl/nerf-rl.gif" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://dannydriess.github.io/" target="_blank">Danny Driess</a>,
				<a href="https://ingmarschubert.com/" target="_blank">Ingmar Schubert</a>,
				<a href="https://www.peteflorence.com/" target="_blank">Pete Florence</a>,
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>, and
				<a href="https://www.user.tu-berlin.de/mtoussai/" target="_blank">Marc Toussaint</a>
				<br>
				<b><font color="black">Reinforcement Learning with Neural Radiance Fields</font></b><br>
				<b><a href="https://neurips.cc/Conferences/2022" target="_blank">NeurIPS 2022</a></b>,
				<a href="https://dannydriess.github.io/nerf-rl/index.html" target="_blank"> <small>[Project & Video]</small></a>
				<a href="https://dannydriess.github.io/papers/22-driess-NeRF-RL-preprint.pdf" target="_blank"> <small>[Paper]</small></a>
				<a href="projects/nerf-rl/nerf-rl.bib" target="_blank"> <small>[BibTex]</small></a><br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/robocraft/robocraft.gif" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://hshi74.github.io/" target="_blank">Haochen Shi</a>*,
				<a href="http://hxu.rocks/" target="_blank">Huazhe Xu</a>*,
				<a href="https://sites.google.com/view/zhiao-huang" target="_blank">Zhiao Huang</a>,
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>, and
				<a href="http://jiajunwu.com" target="_blank">Jiajun Wu</a>
				<br>
				<b><font color="black">RoboCraft: Learning to See, Simulate, and Shape Elasto-Plastic Objects with Graph Networks</font></b><br>
				<b><a href="https://roboticsconference.org/" target="_blank">RSS 2022</a></b>,
				<a href="http://hxu.rocks/robocraft/" target="_blank"> <small>[Project & Video]</small></a>
				<a href="https://arxiv.org/abs/2205.02909" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/hshi74/RoboCraft" target="_blank"> <small>[Code]</small></a>
				<a href="projects/robocraft/robocraft.bib" target="_blank"> <small>[BibTex]</small></a><br>
				Abridged in <b>ICRA 2022</b> workshop on Representing and Manipulating Deformable Objects <a href="https://deformable-workshop.github.io/icra2022/" target="_blank"><small>[Link]</small></a><br>
				<small>Covered by</small>
				<a href="https://news.mit.edu/2022/robots-play-play-dough-0623" target="_blank"> <small>[MIT News]</small></a>
				<a href="https://www.newscientist.com/article/2325970-ai-powered-robot-learned-to-make-letters-out-of-play-doh-on-its-own/" target="_blank"> <small>[NewScientist]</small></a>
				<a href="https://techcrunch.com/2022/06/23/a-quick-trip-to-mars/" target="_blank"> <small>[TechCrunch]</small></a>
				<a href="https://hai.stanford.edu/news/training-robot-shape-letters-play-doh" target="_blank"> <small>[Stanford HAI]</small></a>

				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/TRO_multiobj_manip/TRO_multiobj_manip.jpg" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://scholar.google.com/citations?user=0KfKHOsAAAAJ&hl=en" target="_blank">Zherong Pan</a>,
				<a href="https://andyzeng.github.io/" target="_blank">Andy Zeng</a>,
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>,
				<a href="https://arc-l.github.io/" target="_blank">Jingjin Yu</a>, and
				<a href="https://kkhauser.web.illinois.edu/" target="_blank">Kris Hauser</a>
				<br>
				<b><font color="black">Algorithms and Systems for Manipulating Multiple Objects</font></b><br>
				<b><a href="https://www.ieee-ras.org/publications/t-ro" target="_blank">T-RO 2022</a></b>,
				<a href="https://ieeexplore.ieee.org/abstract/document/9893496" target="_blank"> <small>[Paper]</small></a>
				<a href="projects/TRO_multiobj_manip/TRO_multiobj_manip.bib" target="_blank"> <small>[BibTex]</small></a><br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/diffskill/DiffSkill-compressed.gif" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://xingyu-lin.github.io/" target="_blank">Xingyu Lin</a>,
				<a href="https://sites.google.com/view/zhiao-huang" target="_blank">Zhiao Huang</a>,
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>,
				<a href="https://web.mit.edu/cocosci/josh.html" target="_blank">Joshua B. Tenenbaum</a>,
				<a href="https://davheld.github.io/" target="_blank">David Held</a>, and
				<a href="http://people.csail.mit.edu/ganchuang/" target="_blank">Chuang Gan</a>
				<br>
				<b><font color="black">DiffSkill: Skill Abstraction from Differentiable Physics for Deformable Object Manipulations with Tools</font></b><br>
				<b><a href="https://iclr.cc/Conferences/2022" target="_blank">ICLR 2022</a></b>,
				<a href="https://xingyu-lin.github.io/diffskill/" target="_blank"> <small>[Project]</small></a>
				<a href="https://openreview.net/forum?id=Kef8cKdHWpP" target="_blank"> <small>[Paper]</small></a>
				<a href="projects/diffskill/DiffSkill.bib" target="_blank"> <small>[BibTex]</small></a>
				<br>
				<small>Covered by</small>
				<a href="https://news.mit.edu/2022/robotic-deformable-object-0331" target="_blank"> <small>[MIT News]</small></a>
				<a href="https://techcrunch.com/2022/03/31/better-learning-through-complex-dough-manipulation/" target="_blank"> <small>[TechCrunch]</small></a>
				<a href="https://bdtechtalks.com/2022/05/09/diffskill-robotics-deformable-object-manipulation/" target="_blank"> <small>[TechTalks]</small></a>
				<br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/DAIS/DAIS.png" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://www.csail.mit.edu/person/lujie-yang" target="_blank">Lujie Yang</a>,
				<a href="https://kzhang66.github.io/" target="_blank">Kaiqing Zhang</a>,
				<a href="https://alexandreamice.github.io/" target="_blank">Alexandre Amice</a>,
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>, and
				<a href="https://groups.csail.mit.edu/locomotion/russt.html" target="_blank">Russ Tedrake</a>
				<br>
				<b><font color="black">Discrete Approximate Information States in Partially Observable Environments</font></b><br>
				<b><a href="https://acc2022.a2c2.org/" target="_blank">ACC 2022</a></b>,
				<a href="projects/DAIS/DAIS.pdf" target="_blank"> <small>[Paper]</small></a><br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/nerf-dy/nerf-dy-multiview.gif" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>*,
				<a href="https://people.csail.mit.edu/lishuang/" target="_blank">Shuang Li</a>*,
				<a href="https://vsitzmann.github.io/" target="_blank">Vincent Sitzmann</a>,
				<a href="https://people.csail.mit.edu/pulkitag/" target="_blank">Pulkit Agrawal</a>, and
				<a href="http://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>
				<br>
				<b><font color="black">3D Neural Scene Representations for Visuomotor Control</font></b><br>
				<b><a href="https://www.robot-learning.org/" target="_blank">CoRL 2021</a></b>,
				<a href="https://3d-representation-learning.github.io/nerf-dy/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/abs/2107.04004" target="_blank"> <small>[Paper]</small></a>
				<a href="https://youtu.be/ELPMiifELGc" target="_blank"> <small>[Video]</small></a>
				<a href="https://openreview.net/forum?id=zv3NYgRZ7Qo" target="_blank"> <small>[OpenReview]</small></a>
				<a href="https://3d-representation-learning.github.io/nerf-dy/nerf-dy.bib" target="_blank"> <small>[BibTex]</small></a><br>
				<font color="firebrick"><b>Oral Presentation (Top 6.5%)</b></font><br>
				Abridged in <b>RSS 2021</b> workshop on Visual Learning and Reasoning for Robotics <a href="https://rssvlrr.github.io/" target="_blank"><small>[Link]</small></a><br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/key_dynam/key_dynam.gif" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="http://lucasmanuelli.com/" target="_blank">Lucas Manuelli</a>,
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>,
				<a href="http://www.peteflorence.com/" target="_blank">Pete Florence</a>, and
				<a href="https://groups.csail.mit.edu/locomotion/russt.html" target="_blank">Russ Tedrake</a>
				<br>
				<b><font color="black">Keypoints into the Future: Self-Supervised Correspondence in Model-Based Reinforcement Learning</font></b><br>
				<b><a href="https://www.robot-learning.org/" target="_blank">CoRL 2020</a></b>,
				<a href="https://sites.google.com/view/keypointsintothefuture" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/abs/2009.05085" target="_blank"> <small>[Paper]</small></a>
				<a href="https://www.youtube.com/watch?v=qxC7XS4eFFw" target="_blank"> <small>[Video]</small></a>
				<a href="projects/key_dynam/key_dynam.bib" target="_blank"> <small>[BibTex]</small></a><br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/compkpm/compkpm.gif" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://people.csail.mit.edu/liyunzhu/"><b>Yunzhu Li</b></a>*,
				<a href="http://people.csail.mit.edu/hehaodele/" target="_blank">Hao He</a>*,
				<a href="http://jiajunwu.com" target="_blank">Jiajun Wu</a>,
				<a href="http://people.csail.mit.edu/dina/" target="_blank">Dina Katabi</a>, and
				<a href="https://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>
				<br>
				<b><font color="black">Learning Compositional Koopman Operators for Model-Based Control</font></b><br>
				<b><a href="https://iclr.cc/Conferences/2020" target="_blank">ICLR 2020</a></b>,
				<a href="http://koopman.csail.mit.edu/" target="_blank"> <small>[Project]</small></a>
				<a href="https://openreview.net/forum?id=H1ldzA4tPr" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/YunzhuLi/CompositionalKoopmanOperators" target="_blank"><small>[Code]</small></a>
				<a href="projects/compkpm/compkpm.bib" target="_blank"> <small>[BibTex]</small></a>
				<a href="https://youtu.be/MnXo_hjh1Q4" target="_blank"> <small>[Video]</small></a>
				<a href="http://koopman.csail.mit.edu/poster.pdf" target="_blank"> <small>[Poster]</small></a><br>
				<font color="firebrick"><b>Spotlight Presentation (Top 6.0%)</b></font><br>
				Abridged in <b>NeurIPS 2019</b> workshop on Graph Representation Learning <a href="https://grlearning.github.io/papers/" target="_blank"><small>[Link]</small></a>
				
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/dpi/dpi.png" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>,
				<a href="http://jiajunwu.com" target="_blank">Jiajun Wu</a>,
				<a href="https://groups.csail.mit.edu/locomotion/russt.html" target="_blank">Russ Tedrake</a>,
				<a href="https://web.mit.edu/cocosci/josh.html" target="_blank">Joshua B. Tenenbaum</a>, and
				<a href="http://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>
				<br>
				<b><font color="black">Learning Particle Dynamics for Manipulating Rigid Bodies, Deformable Objects, and Fluids</font></b><br>
				<b><a href="https://iclr.cc/Conferences/2019" target="_blank">ICLR 2019</a></b>,
				<a href="http://dpi.csail.mit.edu" target="_blank"> <small>[Project]</small></a>
				<a href="http://dpi.csail.mit.edu/dpi-paper.pdf" target="_blank"> <small>[PDF]</small></a>
				<a href="https://github.com/YunzhuLi/DPI-Net" target="_blank"> <small>[Code]</small></a>
				<a href="http://dpi.csail.mit.edu/dpi.bib" target="_blank"> <small>[BibTex]</small></a>
				<a href="projects/dpi/dpi-poster.pdf" target="_blank"><small>[Poster]</small></a>
				<a href="https://www.youtube.com/watch?v=FrPpP7aW3Lg" target="_blank"> <small>[Video]</small></a><br>

				<small>Covered by</small>
				<a href="http://news.mit.edu/2019/robots-object-manipulation-particle-simulator-0417" target="_blank"> <small>[MIT News]</small></a>
				<a href="https://www.engadget.com/2019/04/21/mit-particle-simulator-helps-robots-make-sushi/" target="_blank"> <small>[Engadget]</small></a>
				<a href="https://news.developer.nvidia.com/laying-the-foundation-for-better-object-manipulation-in-robotics/" target="_blank"> <small>[NVIDIA Developer]</small></a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/propnet/propnet-1.png" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>,
				<a href="http://jiajunwu.com" target="_blank">Jiajun Wu</a>,
				<a href="http://people.csail.mit.edu/junyanz/" target="_blank">Jun-Yan Zhu</a>,
				<a href="https://web.mit.edu/cocosci/josh.html" target="_blank">Joshua B. Tenenbaum</a>,
				<a href="http://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>, and
				<a href="https://groups.csail.mit.edu/locomotion/russt.html" target="_blank">Russ Tedrake</a>
				<br>
				<b><font color="black">Propagation Networks for Model-Based Control Under Partial Observation</font></b><br>
				<b><a href="https://www.icra2019.org/" target="_blank">ICRA 2019</a></b>,
				<a href="http://propnet.csail.mit.edu" target="_blank"> <small>[Project]</small></a>
				<a href="http://propnet.csail.mit.edu/propnet-paper.pdf" target="_blank"> <small>[PDF]</small></a>
				<a href="https://github.com/YunzhuLi/PropNet" target="_blank"><small>[Code]</small></a>
				<a href="http://propnet.csail.mit.edu/propnet.bib" target="_blank"> <small>[BibTex]</small></a>
				<a href="https://www.youtube.com/watch?v=ZAxHXegkz48" target="_blank"> <small>[Video]</small></a>
				</div>
			</div><hr>


			<br>
			<div id="phys" style="padding-top: 80px; margin-top: -80px;">
				<h5>Physical Scene Understanding</h5>
			</div><br>

   			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/comphy/ComPhy.jpg" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://zfchenunique.github.io/" target="_blank">Zhenfang Chen</a>,
				<a href="https://scholar.google.com/citations?user=SwxS_JkAAAAJ&hl=en" target="_blank">Kexin Yi</a>,
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>,
				<a href="https://dingmyu.github.io/" target="_blank">Mingyu Ding</a>,
				<a href="http://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>,
				<a href="https://web.mit.edu/cocosci/josh.html" target="_blank">Joshua B. Tenenbaum</a>, and
				<a href="http://people.csail.mit.edu/ganchuang/" target="_blank">Chuang Gan</a>
				<br>
				<b><font color="black">ComPhy: Compositional Physical Reasoning of Objects and Events from Videos</font></b><br>
				<b><a href="https://iclr.cc/Conferences/2022" target="_blank">ICLR 2022</a></b>,
				<a href="https://comphyreasoning.github.io/" target="_blank"> <small>[Project]</small></a>
				<a href="https://openreview.net/forum?id=PgNEYaIc81Q" target="_blank"> <small>[Paper]</small></a>
				<a href="projects/comphy/ComPhy.bib" target="_blank"> <small>[BibTex]</small></a><br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/v-cdn/v-cdn.gif" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>,
				<a href="http://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>,
				<a href="http://tensorlab.cms.caltech.edu/users/anima/" target="_blank">Animashree Anandkumar</a>,
				<a href="https://homes.cs.washington.edu/~fox/" target="_blank">Dieter Fox</a>, and
				<a href="https://animesh.garg.tech/" target="_blank">Animesh Garg</a>
				<br>
				<b><font color="black">Causal Discovery in Physical Systems from Videos</font></b><br>
				<b><a href="https://nips.cc/Conferences/2020" target="_blank">NeurIPS 2020</a></b>,
				<a href="https://yunzhuli.github.io/www/V-CDN/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/abs/2007.00631" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/pairlab/v-cdn" target="_blank"> <small>[Code]</small></a>
				<a href="https://www.youtube.com/watch?v=hRsCt8xLn_8" target="_blank"> <small>[Video]</small></a>
				<a href="https://yunzhuli.github.io/www/V-CDN/poster.pdf" target="_blank"> <small>[Poster]</small></a>
				<a href="https://yunzhuli.github.io/www/V-CDN/V-CDN.bib" target="_blank"> <small>[BibTex]</small></a>
			  <br>	
				<small>Covered by</small>
				<a href="https://venturebeat.com/2020/07/02/ai-system-learns-to-model-how-fabrics-interact-by-watching-videos/" target="_blank"> <small>[VentureBeat]</small></a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/physical_scene_graphs/physical_scene_graphs.jpg" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://scholar.google.com/citations?user=uYbkEzYAAAAJ&hl=en" target="_blank">Daniel M. Bear</a>,
				<a href="https://scholar.google.com/citations?user=YM4x068AAAAJ&hl=en" target="_blank">Chaofei Fan</a>,
				<a href="https://scholar.google.com/citations?user=GADXPDcAAAAJ&hl=en" target="_blank">Damian Mrowca</a>,
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>,
				<a href="https://bcs.mit.edu/users/altersmitedu" target="_blank">Seth Alter</a>,
				<a href="https://sites.google.com/site/anayebihomepage/" target="_blank">Aran Nayebi</a>,
				<a href="https://bcs-r1.mit.edu/users/jeremyesmitedu" target="_blank">Jeremy Schwartz</a>,
				<a href="http://vision.stanford.edu/people.html" target="_blank">Li Fei-Fei</a>,
				<a href="http://jiajunwu.com" target="_blank">Jiajun Wu</a>,
				<a href="https://web.mit.edu/cocosci/josh.html" target="_blank">Joshua B. Tenenbaum</a>, and
				<a href="http://stanford.edu/~yamins/" target="_blank">Daniel L. K. Yamins</a>
				<br>
				<b><font color="black">Learning Physical Graph Representations from Visual Scenes</font></b><br>
				<b><a href="https://nips.cc/Conferences/2020" target="_blank">NeurIPS 2020</a></b>,
				<a href="https://neuroailab.github.io/physical-scene-graphs/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/abs/2006.12373" target="_blank"> <small>[Paper]</small></a>
				<a href="projects/physical_scene_graphs/physical_scene_graphs.bib" target="_blank"> <small>[BibTex]</small></a><br>
				<font color="firebrick"><b>Oral Presentation (Top 1.1%)</b></font><br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/visual_grounding/visual_grounding.gif" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>,
				<a href="https://scholar.google.com/citations?user=Rxb7o6IAAAAJ&hl=en" target="_blank">Toru Lin</a>*,
				<a href="https://scholar.google.com/citations?user=SwxS_JkAAAAJ&hl=en" target="_blank">Kexin Yi</a>*,
				<a href="https://scholar.google.com/citations?user=uYbkEzYAAAAJ&hl=en" target="_blank">Daniel M. Bear</a>,
				<a href="http://stanford.edu/~yamins/" target="_blank">Daniel L. K. Yamins</a>,
				<a href="http://jiajunwu.com" target="_blank">Jiajun Wu</a>,
				<a href="https://web.mit.edu/cocosci/josh.html" target="_blank">Joshua B. Tenenbaum</a>, and
				<a href="http://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>
				<br>
				<b><font color="black">Visual Grounding of Learned Physical Models</font></b><br>
				<b><a href="https://icml.cc/Conferences/2020" target="_blank">ICML 2020</a></b>,
				<a href="http://visual-physics-grounding.csail.mit.edu/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/abs/2004.13664" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/YunzhuLi/VGPL" target="_blank"><small>[Code]</small></a>
				<a href="projects/visual_grounding/vgpl.bib" target="_blank"> <small>[BibTex]</small></a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/clevrer/clevrer.gif" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://scholar.google.com/citations?user=SwxS_JkAAAAJ&hl=en" target="_blank">Kexin Yi</a>*,
				<a href="http://people.csail.mit.edu/ganchuang/" target="_blank">Chuang Gan</a>*,
				<a href="https://people.csail.mit.edu/liyunzhu/"><b>Yunzhu Li</b></a>,
				<a href="https://sites.google.com/site/pushmeet/" target="_blank">Pushmeet Kohli</a>,
				<a href="http://jiajunwu.com" target="_blank">Jiajun Wu</a>,
				<a href="https://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>, and
				<a href="https://web.mit.edu/cocosci/josh.html" target="_blank">Joshua B. Tenenbaum</a>
				<br>
				<b><font color="black">CLEVRER: Collision Events for Video Representation and Reasoning</font></b><br>
				<b><a href="https://iclr.cc/Conferences/2020" target="_blank">ICLR 2020</a></b>,
				<a href="http://clevrer.csail.mit.edu/" target="_blank"> <small>[Project]</small></a>
				<a href="https://openreview.net/forum?id=HkxYzANYDB" target="_blank"> <small>[Paper]</small></a>
				<a href="projects/clevrer/clevrer.bib" target="_blank"> <small>[BibTex]</small></a><br>
				<font color="firebrick"><b>Spotlight Presentation (Top 6.0%)</b></font><br>

				<small>Covered by</small>
				<a href="https://www.youtube.com/watch?v=bVXPnP8k6yo" target="_blank"> <small>[Two Minute Papers]</small></a>
				<a href="https://www.technologyreview.com/2020/03/06/905479/ai-neuro-symbolic-system-reasons-like-child-deepmind-ibm-mit/" target="_blank"> <small>[MIT Technology Review]</small></a>
				<a href="https://www.wired.com/story/ai-smart-cant-grasp-cause-effect/" target="_blank"> <small>[WIRED]</small></a>
				<a href="https://venturebeat.com/2020/02/26/researchers-apply-developmental-psychology-to-ai-model-that-predicts-object-relationships/" target="_blank"> <small>[VentureBeat]</small></a>
				
				</div>
			</div><hr>


			<br>
			<div id="multi" style="padding-top: 80px; margin-top: -80px;">
				<h5>Multi-Modal Perception</h5>
			</div><br>

      <div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/objectfolderbenchmark/objectfolderbenchmark.jpg" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://ai.stanford.edu/~rhgao/" target="_blank">Ruohan Gao</a>*,
        <a href="https://dou-yiming.github.io/" target="_blank">Yiming Dou</a>*,
        <a href="https://haolirobo.github.io/" target="_blank">Hao Li</a>*,
				<a href="https://tanmay-agarwal.com/" target="_blank">Tanmay Agarwal</a>,
        <a href="http://web.stanford.edu/~bohg/" target="_blank">Jeannette Bohg</a>,
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>,
        <a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>, and
				<a href="http://jiajunwu.com" target="_blank">Jiajun Wu</a>
				<br>
				<b><font color="black">The ObjectFolder Benchmark: Multisensory Learning with Neural and Real Objects</font></b><br>
				<b><a href="https://cvpr2023.thecvf.com/" target="_blank">CVPR 2023</a></b>,
				<a href="https://objectfolder.stanford.edu/" target="_blank"> <small>[Project]</small></a>
				<a href="https://www.objectfolder.org/swan_vis/" target="_blank"> <small>[Demo]</small></a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/ActionSense/ActionSense.jpg" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://www.josephdelpreto.com/" target="_blank">Joseph DelPreto</a>*,
				<a href="https://chaoliu.tech/" target="_blank">Chao Liu</a>*,
				<a href="https://yyueluo.com/" target="_blank">Yiyue Luo</a>,
        			<a href="https://www.csail.mit.edu/person/michael-foshey" target="_blank">Michael Foshey</a>,
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>,
				<a href="http://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>,
        			<a href="https://cdfg.mit.edu/wojciech" target="_blank">Wojciech Matusik</a>, and
				<a href="https://danielarus.csail.mit.edu/" target="_blank">Daniela Rus</a>
				<br>
				<b><font color="black">ActionSense: A Multimodal Dataset and Recording Framework for Human Activities Using Wearable Sensors in a Kitchen Environment</font></b><br>
				<b><a href="https://neurips.cc/Conferences/2022" target="_blank">NeurIPS 2022 Datasets and Benchmarks</a></b>,
				<a href="https://action-net.csail.mit.edu/" target="_blank"> <small>[Project & Video]</small></a>
				<a href="https://openreview.net/forum?id=olvz0gAdGOX" target="_blank"> <small>[Paper]</small></a>
				<a href="projects/ActionSense/ActionSense.bib" target="_blank"> <small>[BibTex]</small></a><br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/senstextile/senstextile.jpg" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
        			<a href="https://yyueluo.com/" target="_blank">Yiyue Luo</a>,
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>,
        			<a href="https://pratyushasharma.github.io/" target="_blank">Pratyusha Sharma</a>,
        			<a href="https://www.csail.mit.edu/person/wan-shou" target="_blank">Wan Shou</a>,
        			<a href="http://people.csail.mit.edu/kuiwu" target="_blank">Kui Wu</a>,
        			<a href="https://www.csail.mit.edu/person/michael-foshey" target="_blank">Michael Foshey</a>,
        			<a href="https://www.csail.mit.edu/person/beichen-li" target="_blank">Beichen Li</a>,
        			<a href="http://www-mtl.mit.edu/wpmu/tpalacios/" target="_blank">Tomas Palacios</a>,
        			<a href="https://groups.csail.mit.edu/vision/torralbalab/" target="_blank">Antonio Torralba</a>, and
        			<a href="https://cdfg.mit.edu/wojciech" target="_blank">Wojciech Matusik</a>
				<br>
				<b><font color="black">Learning Human-environment Interactions using Conformal Tactile Textiles</font></b><br>
				<b><a href="https://www.nature.com/natelectron/" target="_blank">Nature Electronics</a></b> 4, 193–201 (2021),
				<font color="firebrick"><b>5-year Impact Factor: 33.695</b></font>
				<br>
				<a href="http://senstextile.csail.mit.edu/" target="_blank"> <small>[Project]</small></a>
				<a href="https://www.nature.com/articles/s41928-021-00558-0" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/YunzhuLi/senstextile" target="_blank"> <small>[Code]</small></a>
				<a href="projects/senstextile/senstextile.bib" target="_blank"> <small>[BibTex]</small></a>
				<br>
				<small>Featured on the</small>
				<a href="https://www.nature.com/natelectron/volumes/4/issues/3" target="_blank"> <small>cover</small></a>
				<small>of the issue.</small>
				<small>Editorial comments</small>
				<a href="https://www.nature.com/articles/s41928-021-00567-z" target="_blank"> <small>[Link]</small></a>
				<br>
				<small>Covered by</small>
				<a href="https://www.nature.com/articles/s41928-021-00560-6" target="_blank"> <small>[Nature Electronics News & Views]</small></a>
				<a href="https://www.csail.mit.edu/news/smart-clothes-can-measure-your-movements" target="_blank"> <small>[MIT CSAIL News]</small></a>
				<a href="https://gizmodo.com/researchers-might-have-finally-cracked-smart-clothing-1846546202" target="_blank"> <small>[Gizmodo]</small></a>
				<a href="https://www.engadget.com/mit-csail-smart-clothes-track-movements-160010512.html" target="_blank"> <small>[Engadget]</small></a>
				<br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/phystouch/phystouch.jpeg" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
					<a href="https://github.com/sjtuzq" target="_blank">Qiang Zhang</a>*,
					<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>*,
					<a href="https://yyueluo.com/" target="_blank">Yiyue Luo</a>,
					<a href="https://showone90.wixsite.com/show" target="_blank">Wan Shou</a>,
					<a href="https://www.csail.mit.edu/person/michael-foshey" target="_blank">Michael Foshey</a>,
					<a href="https://thinklab.sjtu.edu.cn/" target="_blank">Junchi Yan</a>,
					<a href="https://scholar.google.com/citations?user=rRJ9wTJMUB8C&hl=en" target="_blank">Joshua B. Tenenbaum</a>,
					<a href="https://cdfg.mit.edu/wojciech" target="_blank">Wojciech Matusik</a>, and
					<a href="http://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>
					<br>
					<b><font color="black">Dynamic Modeling of Hand-Object Interactions via Tactile Sensing</font></b><br>
					<b><a href="https://www.iros2021.org/" target="_blank">IROS 2021</a></b>,
					<a href="http://phystouch.csail.mit.edu/" target="_blank"> <small>[Project]</small></a>
					<a href="https://arxiv.org/abs/2109.04378" target="_blank"> <small>[Paper]</small></a>
					<a href="https://www.youtube.com/watch?v=rBN5kNOw5Y8" target="_blank"> <small>[Video]</small></a>
					<a href="projects/phystouch/phystouch.bib" target="_blank"> <small>[BibTex]</small></a>
					<br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/intcarpet/logo.jpg" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
        			<a href="https://yyueluo.com/" target="_blank">Yiyue Luo</a>,
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>,
        			<a href="https://www.csail.mit.edu/person/michael-foshey" target="_blank">Michael Foshey</a>,
        			<a href="https://www.csail.mit.edu/person/wan-shou" target="_blank">Wan Shou</a>,
        			<a href="https://pratyushasharma.github.io/" target="_blank">Pratyusha Sharma</a>,
        			<a href="http://www-mtl.mit.edu/wpmu/tpalacios/" target="_blank">Tomas Palacios</a>,
        			<a href="https://groups.csail.mit.edu/vision/torralbalab/" target="_blank">Antonio Torralba</a>, and
        			<a href="https://cdfg.mit.edu/wojciech" target="_blank">Wojciech Matusik</a>
				<br>
				<b><font color="black">Intelligent Carpet: Inferring 3D Human Pose from Tactile Signals</font></b><br>
				<b><a href="http://cvpr2021.thecvf.com/" target="_blank">CVPR 2021</a></b>,
				<a href="http://intcarpet.csail.mit.edu/" target="_blank"> <small>[Project]</small></a>
				<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Luo_Intelligent_Carpet_Inferring_3D_Human_Pose_From_Tactile_Signals_CVPR_2021_paper.pdf" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/yiyueluo/IntelligentCarpet" target="_blank"> <small>[Code]</small></a>
				<a href="https://www.youtube.com/watch?v=U6svj37h2U4" target="_blank"> <small>[Video]</small></a>
				<a href="projects/intcarpet/intcarpet.bib" target="_blank"> <small>[BibTex]</small></a>
				<br>
				<small>Covered by</small>
				<a href="https://news.mit.edu/2021/intelligent-carpet-gives-insight-human-poses-0624" target="_blank"> <small>[MIT News]</small></a>
				<a href="https://www.fastcompany.com/90648670/this-magic-carpet-can-track-your-workout" target="_blank"> <small>[Fast Company]</small></a>
				<br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/stag/stag_lowres.jpg" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://people.csail.mit.edu/subras/" target="_blank">Subramanian Sundaram</a>,
				<a href="https://people.csail.mit.edu/pkellnho/" target="_blank">Petr Kellnhofer</a>,
				<a href="https://people.csail.mit.edu/liyunzhu/"><b>Yunzhu Li</b></a>,
				<a href="https://people.csail.mit.edu/junyanz/" target="_blank">Jun-Yan Zhu</a>,
				<a href="https://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>, and
				<a href="https://people.csail.mit.edu/wojciech/" target="_blank">Wojciech Matusik</a>
				<br>
				<b><font color="black">Learning the Signatures of the Human Grasp Using a Scalable Tactile Glove</font></b><br>
				<b><a href="https://www.nature.com/" target="_blank">Nature</a></b> 569, 698–702 (2019),
				<font color="firebrick"><b>5-year Impact Factor: 54.637</b></font>
				<br>
				<a href="http://stag.csail.mit.edu/" target="_blank"> <small>[Project]</small></a>
				<a href="https://www.nature.com/articles/s41586-019-1234-z" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/Erkil1452/touch" target="_blank"> <small>[Code]</small></a>
				<a href="http://stag.csail.mit.edu/files/sundaram2019stag.bib" target="_blank"> <small>[BibTex]</small></a>,

				<small>Collected by</small>
				<a href="projects/stag/stag_mit_museum.jpg" target="_blank"> <small>MIT Museum</small></a><br>

				<small>Covered by</small>
				<a href="http://news.mit.edu/2019/sensor-glove-human-grasp-robotics-0529" target="_blank"> <small>[MIT News]</small></a>
				<a href="https://www.nature.com/articles/d41586-019-01593-w" target="_blank"> <small>[Nature News & Views]</small></a>
				<a href="https://devicematerialscommunity.nature.com/users/257334-subramanian-sundaram/posts/49420-learning-dexterity-from-humans" target="_blank"> <small>[Nature communities]</small></a>
				<a href="https://www.economist.com/science-and-technology/2019/05/30/improving-robots-grasp-requires-a-new-way-to-measure-it-in-humans" target="_blank"> <small>[The Economist]</small></a>
				<a href="https://www.pbs.org/wgbh/nova/article/electronic-glove-pressure-sensors/" target="_blank"> <small>[PBS NOVA]</small></a>
				<a href="https://www.bbc.co.uk/sounds/play/p079yr9y" target="_blank"> <small>[BBC Radio]</small></a>
				<a href="https://www.newscientist.com/article/2204736-smart-glove-works-out-what-youre-holding-from-its-weight-and-shape/" target="_blank"> <small>[NewScientist]</small></a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/visgel/visgel.jpg" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>,
				<a href="http://people.csail.mit.edu/junyanz/" target="_blank">Jun-Yan Zhu</a>,
				<a href="https://groups.csail.mit.edu/locomotion/russt.html" target="_blank">Russ Tedrake</a>, and
				<a href="http://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>
				<br>
				<b><font color="black">Connecting Touch and Vision via Cross-Modal Prediction</font></b><br>
				<b><a href="http://cvpr2019.thecvf.com/" target="_blank">CVPR 2019</a></b>,
				<a href="http://visgel.csail.mit.edu" target="_blank"> <small>[Project]</small></a>
				<a href="http://visgel.csail.mit.edu/visgel-paper.pdf" target="_blank"> <small>[PDF]</small></a>
				<a href="https://github.com/YunzhuLi/VisGel" target="_blank"><small>[Code]</small></a>
				<a href="http://visgel.csail.mit.edu/visgel.bib" target="_blank"> <small>[BibTex]</small></a><br>

				<small>Covered by</small>
				<a href="http://news.mit.edu/2019/teaching-ai-to-connect-senses-vision-touch-0617" target="_blank"> <small>[MIT News]</small></a>
				<a href="https://www.bbc.com/news/av/technology-48711479/robot-taught-to-feel-objects-by-sight-and-other-news" target="_blank"> <small>[BBC]</small></a>
				<a href="https://www.cnn.com/2019/06/17/us/mit-robot-vision-touch-trnd/index.html" target="_blank"> <small>[CNN]</small></a>
				<a href="https://www.forbes.com/sites/charlestowersclark/2019/06/17/one-step-closer-to-human-intelligence-mit-csail-combine-sight-and-touch-in-ai/#3496256578b6" target="_blank"> <small>[Forbes]</small></a>
				<a href="https://techcrunch.com/2019/06/17/mit-develops-a-system-to-give-robots-more-human-senses/" target="_blank"> <small>[TechCrunch]</small></a>
				<a href="https://www.fastcompany.com/90365007/a-new-robot-can-now-identify-objects-by-touch" target="_blank"> <small>[Fast Company]</small></a>
				<a href="https://www.engadget.com/2019/06/17/robot-identify-sight-touch/" target="_blank"> <small>[Engadget]</small></a>
				</div>
			</div><hr>


			<br>
			<div id="imi" style="padding-top: 80px; margin-top: -80px;">
				<h5>Imitation Learning</h5>
			</div><br>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/infogail/infogail.png" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>,
				<a href="http://tsong.me/" target="_blank">Jiaming Song</a>, and
				<a href="http://cs.stanford.edu/~ermon/" target="_blank">Stefano Ermon</a>
				<br>
				<b><font color="black">InfoGAIL: Interpretable Imitation Learning from Visual Demonstrations</font></b><br>
				<b><a href="https://nips.cc/Conferences/2017" target="_blank">NIPS 2017</a></b>,
				<a href="projects/infogail/infogail-paper.pdf" target="_blank"> <small>[PDF]</small></a>
				<a href="https://github.com/YunzhuLi/InfoGAIL" target="_blank"> <small>[Code]</small></a>
				<a href="projects/infogail/infogail.bib" target="_blank"> <small>[BibTex]</small></a>
				<a href="projects/infogail/infogail-poster.pdf" target="_blank"><small>[Poster]</small></a>
				<a href="https://www.youtube.com/watch?v=YtNPBAW6h5k" target="_blank"> <small>[Video]</small></a>
				</div>
			</div><hr>


			<br>
			<div id="det" style="padding-top: 80px; margin-top: -80px;">
				<h5>Detection & Segmentation</h5>
			</div><br>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/skin/detection_tracking.png" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>*,
				<a href="http://cs.stanford.edu/people/esteva/home/index.html" target="_blank">Andre Esteva</a>*,
				<a href="https://stanford.edu/~kuprel/" target="_blank">Brett Kuprel</a>,
				<a href="https://profiles.stanford.edu/roberto-novoa" target="_blank">Rob Novoa</a>,
				<a href="https://profiles.stanford.edu/justin-ko" target="_blank">Justin Ko</a>, and
				<a href="http://robots.stanford.edu/" target="_blank">Sebastian Thrun</a>
				<br>
				<b><font color="black">Skin Cancer Detection and Tracking using Data Synthesis and Deep Learning</font></b><br>
				<a href="https://nips.cc/Conferences/2016" target="_blank"><b>NIPS 2016</b> Workshop on Machine Learning for Health</a><br>
				<a href="http://w3phiai2017.w3phi.com/" target="_blank"><b>AAAI 2017</b> Joint Workshop on Health Intelligence</a><br>
				<a href="projects/skin/detection_tracking-paper.pdf" target="_blank"> <small>[PDF]</small></a>
				<a href="projects/skin/skin.bib" target="_blank"> <small>[BibTex]</small></a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/conv3d/face_detection_conv3d.png" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://yunzhuli.github.io/"><b>Yunzhu Li</b></a>*,
				Benyuan Sun*,
				<a href="http://www.stat.ucla.edu/~tfwu/" target="_blank">Tianfu Wu</a>, and
				<a href="http://www.idm.pku.edu.cn/staff/wangyizhou/" target="_blank">Yizhou Wang</a>
				<br>
				<b><font color="black">Face Detection with End-to-End Integration of a ConvNet and a 3D Model</font></b><br>
				<b><a href="http://www.eccv2016.org/" target="_blank">ECCV 2016</a></b>,
				<a href="projects/conv3d/face_detection_conv3d-paper.pdf" target="_blank"> <small>[PDF]</small></a>
				<a href="https://github.com/tfwu/FaceDetection-ConvNet-3D" target="_blank"> <small>[Code]</small></a>
				<a href="projects/conv3d/conv3d.bib" target="_blank"> <small>[BibTex]</small></a>
				<a href="projects/conv3d/face_detection_conv3d-poster.pdf" target="_blank"> <small>[Poster]</small></a>
				</div>
			</div><hr>

		</script>

	</div>
	
	
	<br><br>


	<!-- Experience -->
	<div class="container">
		<h3 id="Education" style="padding-top: 80px; margin-top: -80px;">Education</h3>
		<ul>
			<li>
				2017.9 - 2022.8, Massachusetts Institute of Technology<br>
				Doctor of Philosophy in Electrical Engineering and Computer Science
			</li>
			<li>
				2017.9 - 2020.5, Massachusetts Institute of Technology<br>
				Master of Science in Electrical Engineering and Computer Science
			</li>
			<li>
				2013.9 - 2017.7, Peking University<br>
				Bachelor of Science (Summa Cum Laude) in Computer Science
			</li>
		</ul>
	</div><br><br>


	<!-- Experience -->
	<div class="container">
		<h3 id="Experience" style="padding-top: 80px; margin-top: -80px;">Experience</h3>
		<ul>
			<li>
				2017.9 - 2022.8, Research Assistant, MIT Computer Science and Artificial Intelligence Laboratory<br>
				Advisors: Prof. <a href="http://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>, Prof. <a href="https://groups.csail.mit.edu/locomotion/russt.html" target="_blank">Russ Tedrake</a>
			</li>
			<li>
				2021.6 - 2021.8, Research Intern, NVIDIA Robotics Research Lab<br>
				Advisors: Prof. <a href="https://homes.cs.washington.edu/~fox/" target="_blank">Dieter Fox</a>, Prof. <a href="https://ai.stanford.edu/~garg/" target="_blank">Animesh Garg</a>, Dr. <a href="https://research.nvidia.com/person/karl-van-wyk" target="_blank">Karl Van Wyk</a>, Dr. <a href="https://research.nvidia.com/person/yuwei-chao" target="_blank">Yu-Wei Chao</a>
			</li>
			<li>
				2019.6 - 2019.9, Research Intern, NVIDIA Robotics Research Lab<br>
				Advisors: Prof. <a href="https://homes.cs.washington.edu/~fox/" target="_blank">Dieter Fox</a>, Prof. <a href="https://ai.stanford.edu/~garg/" target="_blank">Animesh Garg</a>, Prof. <a href="http://tensorlab.cms.caltech.edu/users/anima/" target="_blank"> Animashree Anandkumar</a>
			</li>

			<li>
				2016.9 - 2017.2, Research Assistant, Stanford Artificial Intelligence Laboratory<br>
				Advisor: Prof. <a href="http://cs.stanford.edu/~ermon/" target="_blank">Stefano Ermon</a>
			</li>
			<li>
				2016.6 - 2016.9, Research Assistant, Stanford Artificial Intelligence Laboratory<br>
				Advisor: Prof. <a href="http://robots.stanford.edu/" target="_blank">Sebastian Thrun</a>
			</li>
			<li>
				2015.3 - 2016.5, Research Assistant, Institute of Digital Media, Peking University<br>
				Advisors: Prof. <a href="http://www.idm.pku.edu.cn/staff/wangyizhou/" target="_blank">Yizhou Wang</a>, Prof. <a href="http://www.stat.ucla.edu/~tfwu/" target="_blank">Tianfu Wu</a>
			</li>
		</ul>
	</div><br><br>


	<!-- Awards -->
	<div class="container">
		<h3 id="Honors" style="padding-top: 80px; margin-top: -80px;">Selected Honors</h3>
		<ul>
			<li>First Place Recipient, Ernst A. Guillemin Master's Thesis Award in Artificial Intelligence and Decision Making at MIT, 2021 <a href="https://www.eecs.mit.edu/2021-eecs-awards/" target="_blank"><small>[Link]</small></a></li>
			<li>Adobe Research Fellowship, 2020 <a href="https://adoberesearch.ctlprojects.com/fellowship/previous-fellowship-award-winners/" target="_blank"><small>[Link]</small></a></li>
			<li>Finalist, Facebook Fellowship, 2021 <a href="https://research.fb.com/blog/2021/04/announcing-the-recipients-of-the-2021-facebook-fellowship-awards/" target="_blank"><small>[Link]</small></a></li>
			<li>Finalist, NVIDIA Graduate Fellowship, 2019, 2020, 2021 <a href="https://www.nvidia.com/en-us/research/graduate-fellowships/" target="_blank"><small>[Link]</small></a></li>
			<li>Outstanding Reviewer, ICCV 2021 (Top 5%) <a href="http://iccv2021.thecvf.com/outstanding-reviewers" target="_blank"><small>[Link]</small></a></li>
			<li>Outstanding Undergraduate Thesis Award, EECS Dept., Peking Univ. (10 in 400)</li>
			<li>Peking Univ. Merit Student Pacemaker (10 in 200)</li>
			<li>Lixin-Tang Scholarship (4 in 400)</li>
		</ul>
	</div><br><br>


	<!-- Service -->
	<div class="container">
		<h3 id="Service" style="padding-top: 80px; margin-top: -80px;">Professional Service</h3>
		<ul>
			<li>Workshop Organizer</li>
				<ul>
					<li><a href="https://imrss2022.github.io/" target="_blank">Implicit Representations for Robotic Manipulation</a> at RSS 2022</li>
					<li><a href="https://www.mair2.com/" target="_blank">Multi-Agent Interaction and Relational Reasoning</a> at ICCV 2021</li>
					<li><a href="https://sites.google.com/nvidia.com/do-sim/" target="_blank">Deformable Object Simulation in Robotics</a> at RSS 2021</li>
					<li><a href="https://www.visionmeetscognition.org/" target="_blank">Vision Meets Cognition</a> at CVPR 2019</li>
				</ul>
			<li>Tutorial Organizer</li>
				<ul>
					<li><a href="https://xiaolonw.github.io/graphnnv3/" target="_blank">Learning Representations via Graph-structured Networks</a> at CVPR 2021</li>
					<li><a href="https://bryanyzhu.github.io/mm-iccv2021/" target="_blank">Multi-Modality Learning from Videos and Beyond</a> at ICCV 2021</li>
				</ul>
			<li>Area Chair: CoRL</li>
			<li>Senior Program Committee Member: AAAI</li>
			<li>Conference Reviewer: ICLR, ICML, NeurIPS, ICCV, CVPR, ECCV, ICRA, IROS, RSS, CoRL, Humanoids</li>
			<li>Program Committee Member: AAAI, UAI</li>
			<li>Journal Reviewer:
				<a href="https://www.computer.org/csdl/journal/tp" target="_blank">TPAMI</a>,
				<a href="https://www.springer.com/journal/11263" target="_blank">IJCV</a>,
				<a href="https://www.ieee-ras.org/publications/t-ro" target="_blank">IEEE T-RO</a>,
				<a href="https://www.ieee-ras.org/publications/ra-l" target="_blank">IEEE RA-L</a>,
				<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=42" target="_blank">IEEE-TMI</a>,
				<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6979" target="_blank">IEEE T-ITS</a>,
				<a href="https://ieeeaccess.ieee.org/" target="_blank">IEEE Access</a>,
				<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=4543165" target="_blank">IEEE-ToH</a>
			</li>
		</ul>
	</div><br>

	<div class="container">
		<hr>
		<center>
			<footer>
				<p>&copy; Stanford University 2023</p>
			</footer>
		</center>
	</div>
	<!-- /container -->

	<!-- Bootstrap core JavaScript -->
	<!-- Placed at the end of the document so the pages load faster -->
	<script>showPubs(1);</script>
	<script>var scroll = new SmoothScroll('a[href*="#"]', {speed: 1000});</script>
	<script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.11.0/umd/popper.min.js" integrity="sha384-b/U6ypiBEHpOf/4+1nzFpr53nxSS+GLCkfwBdFNTxtclqqenISfwAzpKaMNFNmj4" crossorigin="anonymous"></script>
	<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/js/bootstrap.min.js" integrity="sha384-h0AbiXch4ZDo7tp9hKZ4TsHbi047NrKGLO3SEJAg45jXxnGIfYzk4Si90RDIqNm1" crossorigin="anonymous"></script>
</body>

</html>
